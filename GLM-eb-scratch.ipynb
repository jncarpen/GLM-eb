{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def get_LL(self,rate,Y):\n",
    "#         '''log-likelihood function'''\n",
    "#         loglik = np.sum(Y*np.log(rate)-rate)\n",
    "#         return loglik\n",
    "    \n",
    "    ##################################################################################################################\n",
    "    \n",
    "#     # functions from the tutorial; will probably change these to simplify (?)\n",
    "    \n",
    "#     def qu(self,z):\n",
    "#         '''the nonlinearity (w/ softplus)'''\n",
    "#         qu = np.log1p(np.exp(z))\n",
    "        \n",
    "#     def lmb(self,beta0,beta,X):\n",
    "#         '''conditional intensity function'''\n",
    "#         z = beta0 + np.dot(X,beta)\n",
    "#         l = self.qu(z)\n",
    "#         return l\n",
    "    \n",
    "#     def penalty(self,alpha,beta):\n",
    "#         '''penalty term'''\n",
    "#         P = 0.5 * (1 - alpha) * np.linalg.norm(beta, 2) ** 2 + \\\n",
    "#         alpha * np.linalg.norm(beta, 1)\n",
    "#         return P\n",
    "    \n",
    "#     def loss(self,beta0, beta, reg_lambda, X, y):\n",
    "#         '''define objective function for elastic net'''\n",
    "#         L = logL(beta0, beta, X, y)\n",
    "#         P = self.penalty(beta)\n",
    "#         J = -L + reg_lambda * P\n",
    "#         return J\n",
    "    \n",
    "#     def grad_L2loss(self,beta0, beta, reg_lambda, X, y):\n",
    "#         z = beta0 + np.dot(X, beta)\n",
    "#         s = expit(z)\n",
    "#         q = self.qu(z)\n",
    "#         grad_beta0 = np.sum(s) - np.sum(y * s / q)\n",
    "#         grad_beta = np.transpose(np.dot(np.transpose(s), X) -\n",
    "#                                  np.dot(np.transpose(y * s / q), X)) + \\\n",
    "#         reg_lambda * (1 - alpha) * beta\n",
    "#         return grad_beta0, grad_beta\n",
    "    \n",
    "#     def hessian_loss(beta0, beta, alpha, reg_lambda, X, y):\n",
    "#         z = beta0 + np.dot(X, beta)\n",
    "#         q = qu(z)\n",
    "#         s = expit(z)\n",
    "#         grad_s = s * (1-s)\n",
    "#         grad_s_by_q = grad_s/q - s/(q * q)\n",
    "#         hess_beta0 = np.sum(grad_s) - np.sum(y * grad_s_by_q)\n",
    "#         hess_beta = np.transpose(np.dot(np.transpose(grad_s), X * X)\n",
    "#                                 - np.dot(np.transpose(y * grad_s_by_q), X * X))\\\n",
    "#                                 + reg_lambda * (1-alpha)\n",
    "#         return hess_beta0, hess_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm = GLM(distr='poisson', score_metric='pseudo_R2', reg_lambda=0.01)\n",
    "XX = X_train.to_numpy();\n",
    "yy = y_train.to_numpy(); yy=yy[:,0]\n",
    "glm.fit(XX,yy)\n",
    "\n",
    "yhat = glm.predict(X_test.to_numpy())\n",
    "pseudo_R2 = glm.score(X_test.to_numpy(), y_test.to_numpy())\n",
    "print('Pseudo R^2 is %.3f' % pseudo_R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ####################### DEPRECIATE #######################\n",
    "#     def test_train(self,df,expr,spiketrain):\n",
    "#         df.insert(loc=0, column='y', value=spiketrain, allow_duplicates=False)\n",
    "#         mask = np.random.rand(len(df)) < 0.8\n",
    "#         df_train = df[mask]\n",
    "#         df_test = df[~mask]\n",
    "#         # split into test and train, and booty\n",
    "#         y_train, X_train = dmatrices(expr, df_train, return_type='dataframe')\n",
    "#         y_test, X_test = dmatrices(expr, df_test, return_type='dataframe')\n",
    "# #         print('Training data set length='+str(len(df_train)))\n",
    "# #         print('Testing data set length='+str(len(df_test)))\n",
    "#         return y_train, X_train, y_test, X_test\n",
    "    \n",
    "#     def test_train_arr(self,df,expr,spiketrain):\n",
    "#         '''return training set as array (not df)'''\n",
    "#         y_train, X_train, y_test, X_test = self.test_train(df,expr,spiketrain)\n",
    "#         y_train_arr = y_train.to_numpy(); y_test_arr = y_test.to_numpy()\n",
    "#         X_test_arr = X_test.to_numpy(); X_train_arr = X_train.to_numpy()\n",
    "#         X_train_arr = X_train_arr[:,1:];  X_test_arr = X_test_arr[:,1:]\n",
    "#         return y_train_arr, X_train_arr, y_test_arr, X_test_arr\n",
    "    \n",
    "    ####################### DEPRECIATE #######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1);\n",
    "ax.plot(test_y[fold]); ax.plot(yhat);\n",
    "ax.set_xlabel('time step'); ax.set_ylabel('y (smoothed rate)');\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 2))\n",
    "_, bin_edges = np.histogram(y,120)\n",
    "ax[0].plot(bin_edges, scipy.stats.norm.pdf(bin_edges, loc=y.mean(), scale=y.std()))\n",
    "ax[0].set_title(r'Distribution of Rates')\n",
    "ax[0].set_xlabel('rate (hz)')\n",
    "ax[0].set_ylabel('hist')\n",
    "\n",
    "sns.kdeplot(y, color='#fcb103', bw=.017,shade=True)\n",
    "ax[1].set_title(r'Distribution of Rates')\n",
    "ax[1].set_xlabel('rate (hz)')\n",
    "ax[1].set_ylabel('probability');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize resutls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(['PE', 'P', 'E'], sse);\n",
    "\n",
    "w_fit = res.x[1:]\n",
    "b_fit = res.x[0]\n",
    "y_hat = g.get_rate(X,w_fit,b_fit)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 8))\n",
    "ax.plot(y[0:10000], label='data');\n",
    "ax.plot(smooth_fr_hat_test[0:10000],label='model');\n",
    "ax.set_title(r'model vs. data')\n",
    "ax.set_xlabel('time (s)')\n",
    "ax.set_ylabel('rate (hz)');\n",
    "ax.legend(loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble for the original code\n",
    "# preamble\n",
    "import scipy.io\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "import scipy.sparse as sps\n",
    "import scipy.stats as stats\n",
    "from pyglmnet import GLM, simulate_glm\n",
    "import scipy as sp\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   ## WANT TO COMBINE OBJECTIVE & GRADIENT FUNCTION\n",
    "#     def grad(self,param,x,y):\n",
    "#         '''compute the gradient of the loss fn'''\n",
    "#         # these are the first order derivatives of the cost\n",
    "#         # function with respect to [each of] the weights\n",
    "#         M, n = x.shape\n",
    "#         y_hat = np.exp(x @ param[1:] + param[0])\n",
    "#         dw = (x.T @ (y_hat - y)) / M\n",
    "#         db = (y_hat - y).mean() \n",
    "#         # we dont really need [db] if the bias term is included as a column of 1's\n",
    "#         # which right now it is currently NOT\n",
    "#         jac = dw; jac=np.append(jac,db);\n",
    "#         return jac\n",
    "\n",
    "    #### UPDATE THIS FUNCTION! ######\n",
    "#     def gradient_descent(self,x, y, w_0, b_0, alpha, num_iter):\n",
    "#         '''minimize loss function w/ gradient descent'''\n",
    "#         w, b = w_0.copy(), b_0\n",
    "#         hist = np.zeros(num_iter)\n",
    "#         M, n = x.shape\n",
    "#         for iter in range(num_iter):\n",
    "#             dw, db = self.grad(x, y, w, b)\n",
    "#             w -= alpha * dw\n",
    "#             b -= alpha * db\n",
    "#             hist[iter] = self.loss(x, y, w, b)\n",
    "#         return w, b, hist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
