{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM-eb \n",
    "@author: Jordan, Ben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import scipy as sp\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLM MODULE\n",
    "# @author: jo carpenter\n",
    "\n",
    "# preamble\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import scipy as sp\n",
    "import statistics\n",
    "from scipy.sparse import spdiags, csr_matrix\n",
    "\n",
    "class glm:\n",
    "    def __init__(self, ST, P, hd):\n",
    "        # remove nans and infinite values\n",
    "        idx_finite = np.where(np.isfinite(P[:,1]))[0]\n",
    "        idx_notnan = np.where(~np.isnan(P[:,1]))[0]\n",
    "        keep_idx = np.intersect1d(idx_finite, idx_notnan)\n",
    "        \n",
    "        self.P = P[keep_idx,:]\n",
    "        self.x = P[keep_idx,1]\n",
    "        self.y = P[keep_idx,2]\n",
    "        self.t = P[keep_idx,0]\n",
    "        self.hd = (hd[keep_idx,0]*np.pi)/180; # 0-2pi\n",
    "        self.dt = P[:,1]-P[:,0]\n",
    "        self.ST = ST # spiketimes (not train)\n",
    "        \n",
    "        \n",
    "    def get_size(self):\n",
    "        '''get size of recording box'''\n",
    "        \n",
    "        boxsz = np.nanmax([np.nanmax(self.x), np.nanmax(self.y)])\n",
    "        \n",
    "        return boxsz\n",
    "    \n",
    "    \n",
    "    \n",
    "    def pos_map(self, nbins=10):\n",
    "        '''design matrix for position variables'''\n",
    "        \n",
    "        boxsz = self.get_size()\n",
    "        bins = np.arange(boxsz/nbins/2, boxsz-boxsz/nbins/2, round(boxsz/nbins))\n",
    "        posgrid = np.zeros((len(self.x), nbins**2))\n",
    "        \n",
    "        for idx,val in enumerate(self.x):\n",
    "            \n",
    "            xvec = np.abs(self.x[idx]-bins); yvec = np.abs(self.y[idx]-bins);\n",
    "            min_x = np.min(xvec)\n",
    "            min_y = np.min(yvec)\n",
    "            idx_x = np.where(xvec == min_x); idx_x = idx_x[0][0];\n",
    "            idx_y = np.where(yvec == min_y); idx_y = idx_y[0][0];\n",
    "            bin_idx = np.ravel_multi_index((idx_y,idx_x), dims=(nbins,nbins), order='C') # a11=0, a12=1, a13=2;\n",
    "            posgrid[idx, bin_idx] = 1;\n",
    "            \n",
    "        return posgrid, bins\n",
    "    \n",
    "    \n",
    "    \n",
    "    def eb_map(self, nbins=10, rp=[75,75]):\n",
    "        '''design matrix for egocentric variables'''\n",
    "        \n",
    "        refx = rp[0]; refy = rp[1];\n",
    "        allo = np.arctan2(refy-self.y, refx-self.x) + (np.pi/2); # add 90 deg\n",
    "        allo[allo<0] = allo[allo<0]+2*np.pi;\n",
    "        ego = allo - self.hd; # shift from 0-2pi\n",
    "        egogrid = np.zeros((len(self.P),nbins));\n",
    "        bins = np.arange(2*np.pi/nbins/2, 2*np.pi-2*np.pi/nbins/2, 2*np.pi/nbins) # 10 bin ctrs\n",
    "        \n",
    "        for idx,val in enumerate(self.P):\n",
    "            \n",
    "            evec = np.abs(ego[idx]-bins)\n",
    "            min_e = np.min(evec)\n",
    "            idx_e = np.where(evec == min_e)\n",
    "            egogrid[idx, idx_e] = 1;\n",
    "            \n",
    "        return egogrid, bins\n",
    "    \n",
    "    \n",
    "    \n",
    "    def conv_spktrain(self, Xx=np.linspace(-4,4,9),\n",
    "                      sigma=2,c=0,defaultST=True,spikeIn=[1,2,3],dt=0.02):\n",
    "        '''get smoothed spiketrain from spiketimes\n",
    "            **kwargs:\n",
    "            spikeTrain- 'False' if user wants self.ST (spiketimes)\n",
    "                        'True' if user wants to use a pre-allocated spiketrain\n",
    "            spikeIn-    use this optional kwarg iff spikeTrain==True\n",
    "        '''\n",
    "        if defaultST==True:\n",
    "            t = self.P[:,0]; dt = self.P[1,0]-self.P[0,0]; # time per frame\n",
    "            boolean_spk = np.logical_and(t[0] <= self.ST, self.ST <= t[-1])\n",
    "            spikes = self.ST[boolean_spk == True]\n",
    "            edgesT = np.linspace(t[0], t[-1], len(t)+1)\n",
    "            binnedSpikes, timeEdges = np.histogram(spikes, edgesT)\n",
    "            \n",
    "        elif defaultST==False:\n",
    "            binnedSpikes = spikeIn\n",
    "        \n",
    "        # remove any nans/infinite values in spiketrain\n",
    "        idx_inf = np.where(~np.isfinite(binnedSpikes))[0]\n",
    "        idx_nan = np.where(np.isnan(binnedSpikes))[0]\n",
    "        replace_idx = np.union1d(idx_inf, idx_nan)\n",
    "        binnedSpikes[replace_idx] = 0\n",
    "        \n",
    "        # convolve w/ gaussian membership function\n",
    "        filt = np.exp((-(Xx-c)**2)/(2*(sigma**2)))\n",
    "        fr = binnedSpikes/dt # rate (hz)\n",
    "        smooth_fr = np.convolve(binnedSpikes, filt, mode='same')\n",
    "        \n",
    "        return smooth_fr, binnedSpikes, filt, dt\n",
    "\n",
    "\n",
    "    \n",
    "    def get_speed(self):\n",
    "        '''get speed of the animal (cm*s^-2)'''\n",
    "        \n",
    "        t=self.P[:,0]\n",
    "        x=self.P[:,1]\n",
    "        y=self.P[:,2]\n",
    "        ntime = len(t)\n",
    "        v = np.zeros((ntime,1));\n",
    "        \n",
    "        for idx in range(1,ntime-1):\n",
    "            \n",
    "            v[idx,0] = np.sqrt((x[idx+1]-x[idx-1])**2 + (y[idx+1]-y[idx-1])**2)/(t[idx+1]-t[idx-1])    \n",
    "        v[0,0] = v[1,0]; v[-1,0] = v[-2,0] # pad the array\n",
    "        \n",
    "        return v\n",
    "    \n",
    "    \n",
    "    \n",
    "    def speed_threshold(self,posgrid,ebgrid,spiketrain):\n",
    "        \n",
    "        v = self.get_speed()\n",
    "        maxspeed=50; minspeed=4\n",
    "        inbounds = np.logical_and((v<=maxspeed), (v>=minspeed))\n",
    "        inbounds = np.where(inbounds==True); inbounds = inbounds[0]\n",
    "        posgrid = posgrid[inbounds,:]\n",
    "        ebgrid = ebgrid[inbounds,:]\n",
    "        spiketrain = spiketrain[inbounds]\n",
    "        \n",
    "        return posgrid, ebgrid, spiketrain\n",
    "    \n",
    "    \n",
    "    \n",
    "    def squish_statemat(self, spiketrain, stateIn, modelType='PE'):\n",
    "        '''squish state matrix for 2-variable model (P+EB)\n",
    "            inputs- spiketrain is the speed-thresholded spiketrain'''\n",
    "        \n",
    "        if modelType == 'PE':\n",
    "            posgrid = stateIn[0]; ebgrid = stateIn[1]\n",
    "            ntime,nbins_eb = np.shape(ebgrid)\n",
    "            _,nbins_p = np.shape(posgrid)\n",
    "            A = np.zeros((ntime, nbins_p+nbins_eb)) #P+EB\n",
    "            A[:,0:nbins_p] = posgrid; A[:,nbins_p:] = ebgrid\n",
    "            df = pd.DataFrame(A)\n",
    "            \n",
    "            # name columns & get expression\n",
    "            colnames = [];\n",
    "            expr = 'y ~ '\n",
    "            \n",
    "            for i in range(nbins_p):\n",
    "                val = str(i);\n",
    "                expr = expr + 'P' + val + ' + '\n",
    "                colnames.append('P' + val)\n",
    "                \n",
    "            for i in range(nbins_eb-1):\n",
    "                val = str(i);\n",
    "                expr = expr + 'E' + val + ' + '\n",
    "                colnames.append('E' + val)\n",
    "            expr = expr + 'E9'\n",
    "            colnames.append('E9')\n",
    "            df.columns = colnames\n",
    "            \n",
    "        elif modelType == 'P':\n",
    "            ntime,nbins = np.shape(stateIn)\n",
    "            df = pd.DataFrame(stateIn)\n",
    "            colnames = [];\n",
    "            expr = 'y ~ '\n",
    "            \n",
    "            for i in range(nbins-1):\n",
    "                val = str(i);\n",
    "                expr = expr + 'P' + val + ' + '\n",
    "                colnames.append('P' + val)\n",
    "            expr = expr + 'P99'\n",
    "            colnames.append('P99')\n",
    "            df.columns = colnames\n",
    "            \n",
    "        elif modelType == 'E':\n",
    "            ntime,nbins = np.shape(stateIn)\n",
    "            df = pd.DataFrame(stateIn)\n",
    "            colnames = [];\n",
    "            expr = 'y ~ '\n",
    "            \n",
    "            for i in range(nbins-1):\n",
    "                val = str(i);\n",
    "                expr = expr + 'E' + val + ' + '\n",
    "                colnames.append('E' + val)\n",
    "            expr = expr + 'E9'\n",
    "            colnames.append('E9')\n",
    "            df.columns = colnames\n",
    "            \n",
    "        else:\n",
    "            print('Error: model type must be \"P\", \"E\", or \"PE\"')\n",
    "            \n",
    "        # if you want to do a 20-80 test-train split\n",
    "        # note: make this an option\n",
    "        mask = np.random.rand(len(df)) < 0.8\n",
    "        df_train = df[mask]; df_test = df[~mask]\n",
    "        \n",
    "        # insert [raw] spiketrain into dataframe\n",
    "        df.insert(0, 'y', spiketrain)\n",
    "        \n",
    "        return df,expr\n",
    "    \n",
    "    \n",
    "    \n",
    "    def kfoldSplit(self,nfolds=10):\n",
    "        '''train-test split for k-fold xval\n",
    "            each section is ~1 min'''\n",
    "        \n",
    "        _, spiketrain, _, dt = self.conv_spktrain()\n",
    "        \n",
    "        # calculate number of chunks given session length\n",
    "        howManySeconds = 20\n",
    "        nmins = (len(spiketrain)*dt)/howManySeconds\n",
    "        nchunks = int(round(nmins/nfolds))\n",
    "        nsections = int(nchunks*nfolds)\n",
    "        \n",
    "        # grab indices for k-fold splitting\n",
    "        kfoldIdx = {}\n",
    "        howLong = np.zeros(nfolds)\n",
    "        edges = np.round(np.linspace(1,len(spiketrain)+1,nsections+1))\n",
    "        \n",
    "        for k in range(nfolds):\n",
    "            test_ind = np.floor(np.linspace(int(edges[k]),\n",
    "                    (int(edges[k+1])-1),\n",
    "                    (int(edges[k+1])-1)-int(edges[k])))\n",
    "            \n",
    "            for s in range(1,nchunks):\n",
    "                ind = np.floor(np.linspace(int(edges[k+s*nfolds]),\n",
    "                                  (int(edges[k+s*nfolds+1])-1),\n",
    "                                  (int(edges[k+s*nfolds+1]))-int(edges[k+s*nfolds])))\n",
    "                \n",
    "                test_ind = np.append(test_ind,ind)\n",
    "                \n",
    "            kfoldIdx[k] = test_ind\n",
    "            howLong[k] = len(test_ind)\n",
    "\n",
    "        minArrLen = int(np.min(howLong))\n",
    "        \n",
    "        for k in range(nfolds):\n",
    "            kfoldIdx[k] = kfoldIdx[k][0:minArrLen] # adjust arr. len (w/in .02 s)\n",
    "        \n",
    "        kfoldIdx_df = pd.DataFrame.from_dict(kfoldIdx)\n",
    "        kfoldIdx_df = kfoldIdx_df.astype(int) # for idxing purposes\n",
    "        \n",
    "        return kfoldIdx, kfoldIdx_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    def kfoldOptim(self,kfoldIdx_df,statemat,modelType='PE'):\n",
    "        '''kfoldIdx_df can be retrieved from self.kfoldSplit()'''\n",
    "        \n",
    "        # intialize output structures\n",
    "        _,nfolds=np.shape(kfoldIdx_df)\n",
    "        k_vec = np.arange(nfolds)\n",
    "        kres = {}\n",
    "        train_y = {}\n",
    "        train_x = {}\n",
    "        test_y = {}\n",
    "        test_x = {}\n",
    "        train_y_raw = {}\n",
    "        test_y_raw = {}\n",
    "        \n",
    "        for foldnum in range(nfolds):\n",
    "            k_vec_train = np.delete(k_vec, np.where(k_vec == foldnum))\n",
    "            idx_test = kfoldIdx_df.loc[:,foldnum].to_numpy()\n",
    "            idx_train = []\n",
    "            \n",
    "            # squeeze other nfolds-1 folds into one vector\n",
    "            for i,v in enumerate(k_vec_train):\n",
    "                nextRow = kfoldIdx_df.loc[:,v].to_numpy()\n",
    "                idx_train = np.append(idx_train,nextRow) \n",
    "            idx_train = idx_train.astype(int)\n",
    "            \n",
    "            # train-test statemats\n",
    "            df_test = statemat.loc[idx_test,:].dropna()\n",
    "            df_train = statemat.loc[idx_train,:].dropna()\n",
    "            y_test_raw = df_test['y'].to_numpy(dtype='int64')\n",
    "            y_train_raw = df_train['y'].to_numpy(dtype='int64')\n",
    "            \n",
    "            # smooth firing rates\n",
    "            y_test, _, _, _ = self.conv_spktrain(defaultST=False,spikeIn=y_test_raw)\n",
    "            y_train, _, _, _ = self.conv_spktrain(defaultST=False,spikeIn=y_train_raw) \n",
    "            \n",
    "            # put smoothed firing rates back into dataframe\n",
    "            df_test[df_test.columns[0]] = y_test\n",
    "            df_train[df_train.columns[0]] = y_train\n",
    "            \n",
    "            # test/train arrays\n",
    "            X_test = df_test[df_test.columns[1:]].to_numpy(); \n",
    "            y_test = df_test[df_test.columns[0]].to_numpy()\n",
    "            X_train = df_train[df_train.columns[1:]].to_numpy()\n",
    "            y_train = df_train[df_train.columns[0]].to_numpy()\n",
    "            \n",
    "            # set some initial parameters\n",
    "            M,n = np.shape(X_train)\n",
    "            w_0 = np.ones((n, ))*1e-3\n",
    "            b_0 = 1\n",
    "            # alpha = 0.001 (can't remember when we use this)\n",
    "            \n",
    "            # get parameters & jacobian (1st order derivatives of loss fn)\n",
    "            data,param = self.getDataParam(X_train,y_train,w_0,b_0,modelType)\n",
    "            \n",
    "            # not being used ** (depreciated right now)\n",
    "            # jac = self.grad(param,X_train,y_train)\n",
    "            \n",
    "            # optimize loss function\n",
    "            res = self.bfgs(data,param)\n",
    "            \n",
    "            # package outputs for each fold\n",
    "            kres[foldnum] = res\n",
    "            train_y[foldnum] = y_train; test_y[foldnum] = y_test\n",
    "            train_x[foldnum] = X_train; test_x[foldnum] = X_test\n",
    "            train_y_raw[foldnum] = y_train_raw; \n",
    "            test_y_raw[foldnum] = y_test_raw; \n",
    "            \n",
    "        return kres,train_y, test_y, train_x, test_x, train_y_raw, test_y_raw, data, param\n",
    "    \n",
    "    \n",
    "    \n",
    "    def init_params(self,whichVars={'P', 'E'}):\n",
    "        \n",
    "        if whichVars == {'P', 'E'}: init_param = np.random.randn(110, 1); # * 1e-3\n",
    "        if whichVars == {'P'}: init_param = np.random.randn(100, 1);\n",
    "        if whichVars == {'E'}: init_param = np.random.randn(10, 1);\n",
    "        \n",
    "        return init_param\n",
    "    \n",
    "    \n",
    "    def getDataParam(self,x,y,w,b,modelType='PE'):\n",
    "        '''put param & data in a dictionary'''\n",
    "        \n",
    "        param = np.append(b,w)\n",
    "        data =  ((x, y, modelType))\n",
    "        \n",
    "        return data,param\n",
    "    \n",
    "    \n",
    "    def get_rate(self,x,w,b):\n",
    "        '''conditional intensity function'''\n",
    "        \n",
    "        # note: not normalized by dt (not in Hz)\n",
    "        y_hat = np.exp(x @ w + b)\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "    def loss(self,param,x,y,modelType):\n",
    "        '''objective function'''\n",
    "        # roughness regularizer weights\n",
    "        b_pos = 8e0; b_eb = 5e1;\n",
    "                \n",
    "        M, n = np.shape(x)\n",
    "        \n",
    "        # predicted firing rate\n",
    "        y_hat = np.exp(x @ param[1:] + param[0])\n",
    "        \n",
    "        # compute jacobian (gradient)\n",
    "        dw = (x.T @ (y_hat - y)) / M\n",
    "        db = (y_hat - y).mean()\n",
    "        jac = dw; jac=np.append(jac,db);\n",
    "        \n",
    "        \n",
    "        ## penalize objective fn & gradient\n",
    "        if modelType == 'P':\n",
    "            J_pos, J_pos_g, J_pos_h = self.rough_penalty(param,b_pos,vartype='2D')\n",
    "            \n",
    "            y_hat += J_pos\n",
    "            jac += np.concatenate((np.zeros(1), J_pos_g))\n",
    "            \n",
    "        elif modelType == 'E':\n",
    "            J_eb, J_eb_g, J_eb_h = self.rough_penalty(param,b_eb,vartype='1D-circ')\n",
    "            \n",
    "            y_hat += J_eb\n",
    "            jac += np.concatenate((np.zeros(1), J_eb_g))\n",
    "            \n",
    "        elif modelType == 'PE':\n",
    "            # split parameters for P and E\n",
    "            # @note: this should be soft-coded later\n",
    "            biasterm = param[0]\n",
    "            param_pos = np.append(biasterm, param[1:101])\n",
    "            param_eb = np.append(biasterm, param[101:])\n",
    "\n",
    "            J_pos, J_pos_g, J_pos_h = self.rough_penalty(param_pos,b_pos,vartype='2D')\n",
    "            J_eb, J_eb_g, J_eb_h = self.rough_penalty(param_eb,b_eb,vartype='1D-circ')\n",
    "\n",
    "            y_hat += J_pos\n",
    "            y_hat += J_eb\n",
    "            jac += np.concatenate((np.zeros(1), J_pos_g, J_eb_g))\n",
    "                                \n",
    "        else:\n",
    "            print('error: enter valid model type (\"E\", \"P\", \"PE\")')\n",
    "        \n",
    "        #negative log likelihood for possion where yhat is lambda\n",
    "        y_hat_log = y_hat\n",
    "        result = np.where(y_hat_log == 0, y_hat_log, 1e-10)\n",
    "        logTerm = np.log(result, out=result, where=result>0)\n",
    "        error = (y_hat - logTerm * y).mean()\n",
    "        \n",
    "        # only take the real part of the jacobian\n",
    "        jac = np.real(jac)\n",
    "        \n",
    "        return [error,jac]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def bfgs(self,data,param):\n",
    "        'minimize loss function w/ L-BFGS-B'\n",
    "        \n",
    "        res = sp.optimize.minimize(self.loss, x0=param, args=data, method='L-BFGS-B', jac=True, options={'disp': True})\n",
    "        # options={'gtol': 1e-6, 'disp': True}) # add options \n",
    "        \n",
    "        return res\n",
    "    \n",
    "    \n",
    "    \n",
    "    def rough_penalty(self,param,beta,vartype='1D-circ'):\n",
    "        '''computes roughness penalty\n",
    "\n",
    "        inputs:\n",
    "        >> vartype:\n",
    "            >> '2D', '1D', or '1D-circ'\n",
    "\n",
    "        returns:\n",
    "        >> J: penalty term for objective function\n",
    "        >> J_g: penalty term for gradient (1st order derivatives)\n",
    "        >> J_h: penalty term for Hessian (2nd order derivatives)\n",
    "\n",
    "        '''\n",
    "        param = param[1:] # remove the bias term\n",
    "        numParam = len(param)\n",
    "\n",
    "        if vartype.__contains__('1D'):\n",
    "            data_diag = np.ones(int(numParam))\n",
    "            data_diag = [-data_diag,data_diag]\n",
    "            diags_diag = np.array([0,1]) # diagonals to set\n",
    "            m,n = int(numParam)-1, int(numParam) # shape of resulting matrix\n",
    "            D1 = spdiags(data_diag, diags_diag, m, n).toarray()\n",
    "            DD1 = D1.T @ D1\n",
    "\n",
    "            if vartype.__contains__('circ'):\n",
    "                # to correct the smoothing across first/last bin\n",
    "                DD1[0,:] = np.roll(DD1[1,:],((0, -1)))\n",
    "                DD1[-1,:] = np.roll(DD1[-1,:],((0, 1)))\n",
    "\n",
    "            # penalty terms\n",
    "            J = beta * 0.5 * param.T @ DD1 @ param\n",
    "            J_g = beta * DD1 @ param\n",
    "            J_h = beta * DD1\n",
    "\n",
    "        elif vartype.__contains__('2D'):\n",
    "\n",
    "            data_diag = np.ones(int(np.sqrt(numParam)))\n",
    "            data_diag = [-data_diag,data_diag]\n",
    "            diags_diag = np.array([0,1]) # diagonals to set\n",
    "            m,n = int(np.sqrt(numParam))-1, int(np.sqrt(numParam)) # shape of resulting matrix\n",
    "            D1 = spdiags(data_diag, diags_diag, m, n).toarray()\n",
    "            DD1 = D1.T @ D1\n",
    "\n",
    "            M1 = np.kron(np.eye(int(np.sqrt(numParam))),DD1)\n",
    "            M2 = np.kron(DD1,np.eye(int(np.sqrt(numParam))))\n",
    "            M = (M1 + M2)\n",
    "            \n",
    "            J = beta * 0.5 * param.T @ M @ param\n",
    "            J_g = beta * M @ param\n",
    "            J_h = beta * M \n",
    "        \n",
    "        return J, J_g, J_h\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_stats(self, y, y_hat):\n",
    "        # compare between test fr and model fr\n",
    "        sse = np.sum((y_hat-y)**2);\n",
    "        sst = sum((y-np.mean(y))**2);\n",
    "        varExplain_test = 1-(sse/sst)\n",
    "        r, pval_r = stats.pearsonr(y,y_hat)\n",
    "        return sse, sst, varExplain_test, r, pval_r\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_testFit(self,modelType,kres,train_y,test_y,train_x,test_x,train_y_raw,test_y_raw):\n",
    "        '''get statistics for model fit'''\n",
    "        \n",
    "        # initialize output structures\n",
    "        yhatDict={}\n",
    "        nfolds=len(kres);\n",
    "        sse=np.zeros(nfolds); sst=np.zeros(nfolds)\n",
    "        varExplain_test=np.zeros(nfolds); pearson_r=np.zeros(nfolds)\n",
    "        pearson_pval=np.zeros(nfolds); funval=np.zeros(nfolds)\n",
    "        llh=np.zeros(nfolds)\n",
    "        loss_llh = np.zeros(nfolds)\n",
    "        \n",
    "        for fold in range(nfolds):\n",
    "            # predict model output on *test* data\n",
    "            yhat_raw = self.get_rate(test_x[fold],kres[fold].x[1:],kres[fold].x[0]) # not normalized\n",
    "            yhat, _, _, _ = self.conv_spktrain(defaultST=False,spikeIn=yhat_raw) # normalized by dt (hz)\n",
    "            \n",
    "            # compare between test and model rates\n",
    "            sse[fold] = np.sum((yhat-test_y[fold])**2); #sse\n",
    "            sst[fold]= sum((test_y[fold]-np.mean(test_y[fold]))**2); #sst\n",
    "            varExplain_test[fold] = 1-(sse[fold]/sst[fold]) #varExplained_test\n",
    "            pearson_r[fold],pearson_pval[fold] = stats.pearsonr(test_y[fold],yhat) #pearsonsR,p-val\n",
    "            funval[fold] = kres[fold].fun\n",
    "            yhatDict[fold] = yhat\n",
    "            \n",
    "            #compute llh increase from \"mean firing rate model\"\n",
    "            # NO SMOOTHING is used here\n",
    "            bestp = kres[fold]['x'] # best parameters\n",
    "            n = test_y_raw[fold]\n",
    "            arrFactorial = np.vectorize(math.factorial) # array-wise factorial fn\n",
    "            meanFR_test = np.nanmean(n)\n",
    "\n",
    "            # format the state matrix\n",
    "            b = np.ones((len(test_x[fold]),len(bestp)))\n",
    "            b[:,1:] = test_x[fold]\n",
    "            # get rate of 'mean fr model'\n",
    "            r = np.exp(b @ bestp) # predicted rate (not normalized by dt)\n",
    "            \n",
    "            # compute log-likelihood value for the test data\n",
    "#             log_llh_test_model = np.nansum(r-n*np.log(r)+np.log(arrFactorial(n)))/np.sum(n);\n",
    "#             log_llh_test_mean = np.nansum(meanFR_test-n*np.log(meanFR_test)+np.log(arrFactorial(n)))/np.sum(n);\n",
    "#             log_llh_test = (-log_llh_test_model + log_llh_test_mean);\n",
    "#             log_llh_test = np.log(2)*log_llh_test;\n",
    "#             llh[fold] = log_llh_test\n",
    "            \n",
    "            # akaike info criterion\n",
    "            # log is undefined b/c llh is negative\n",
    "            # AIC = 2*(len(bestp))-2*np.log(log_llh_test);\n",
    "            \n",
    "            # plain LLH (from loss function)\n",
    "            [error,jac] = self.loss(kres[fold].x,test_x[fold],test_y[fold],modelType)\n",
    "            loss_llh[fold] = error;\n",
    "            \n",
    "        # dictionary of statistics describing fit of test data to model\n",
    "        testfit = {\n",
    "#             'llh_test': llh,\n",
    "            'loss_llh': loss_llh,\n",
    "            'sse': sse,\n",
    "            'sst': sst,\n",
    "            'varEx': varExplain_test,\n",
    "            'pearson_r': pearson_r,\n",
    "            'pearson_pval': pearson_pval,\n",
    "            'funval': funval,\n",
    "            'yhat': yhatDict\n",
    "        }\n",
    "        return testfit\n",
    "    \n",
    "    \n",
    "    \n",
    "    def findBestModel(self,allModels,labelDict):\n",
    "        '''modelDict is defined in the script below (incorporate)'''\n",
    "        \n",
    "        numModels = len(allModels)\n",
    "        llh = np.zeros(numModels)\n",
    "        \n",
    "        for model in range(numModels):\n",
    "            T = allModels[model]['testfit']\n",
    "            llh[model] = np.nanmean(T['loss_llh'])\n",
    "        bestModel=np.where(llh==np.max(llh))[0][0]\n",
    "        \n",
    "        print('best model: ' + labelDict[bestModel])\n",
    "        \n",
    "        return llh, bestModel\n",
    "    \n",
    "    \n",
    "    \n",
    "    def plot_llh(self,allModels,labelDict):\n",
    "        '''plot cross-validated\n",
    "        log-likelihood values for all models'''\n",
    "        \n",
    "        # plot log-likelihood values for each model\n",
    "        numModels = len(allModels)\n",
    "        nfolds = len(allModels[0]['kres'].keys())\n",
    "        \n",
    "        llh = {}; whichModel = {}\n",
    "        \n",
    "        for model in range(numModels):\n",
    "            a = allModels[model]['testfit']['loss_llh']\n",
    "            a = a[None].T\n",
    "            llh[model] = a\n",
    "            whichModel[model] = np.ones((nfolds,1))*model\n",
    "\n",
    "#         llh_vstack = np.vstack((llh[0], llh[1], llh[2]))\n",
    "#         whichModel = np.vstack((whichModel[0], whichModel[1], whichModel[2]))\n",
    "#         df = pd.DataFrame(np.hstack((llh_vstack,whichModel)), columns = ['llh','whichModel'])\n",
    "#         ax,llh = sns.swarmplot(x=\"whichModel\", y=\"llh\", data=df);\n",
    "        \n",
    "        return llh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glmmod\n",
    "# should be stored in the same folder *\n",
    "\n",
    "# load & format sample data\n",
    "# filepath = 'sampleData.mat'\n",
    "# mat = scipy.io.loadmat(filepath)\n",
    "# ST = mat['ST']; P = mat['P']; hd = mat['hd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blackstad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load blackstad data\n",
    "filepath = 'C:\\\\Users\\\\17145\\\\OneDrive - NTNU\\\\Documents\\\\methods-project\\\\data_code\\\\curatedUnitsJS\\\\passuPython.mat'\n",
    "filepath_pos = 'C:\\\\Users\\\\17145\\\\OneDrive - NTNU\\\\Documents\\\\methods-project\\\\data_code\\\\curatedUnitsJS\\\\P.mat'\n",
    "mat = scipy.io.loadmat(filepath)\n",
    "\n",
    "import mat73\n",
    "mat_pos = mat73.loadmat(filepath_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### viz parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (4,2)\n",
    "plt.rc('axes', labelsize=10); plt.rc('axes', titlesize=10)\n",
    "plt.style.use('ggplot'); plt.rc('font', size=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### format variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_all = mat['Spk'][0]\n",
    "HD_all = mat['HeadDir'][0]\n",
    "RP_all = mat['RP'][0]\n",
    "P_all = mat_pos['PP']\n",
    "whichSess = mat_pos['whichSess']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17145\\.conda\\envs\\tdt4195\\lib\\site-packages\\pandas\\core\\indexing.py:1418: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: PE\n",
      "unit 1\n",
      "best model: PE\n",
      "unit 2\n",
      "best model: PE\n",
      "unit 3\n",
      "best model: PE\n",
      "unit 4\n",
      "best model: PE\n",
      "unit 5\n",
      "best model: PE\n",
      "unit 6\n",
      "best model: PE\n",
      "unit 7\n",
      "best model: PE\n",
      "unit 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17145\\.conda\\envs\\tdt4195\\lib\\site-packages\\ipykernel_launcher.py:515: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "C:\\Users\\17145\\.conda\\envs\\tdt4195\\lib\\site-packages\\scipy\\stats\\stats.py:3399: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: PE\n",
      "unit 9\n",
      "best model: PE\n",
      "unit 10\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 66049 is out of bounds for axis 0 with size 66049",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-1ec66281f957>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# initialize class instance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mST\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# prepare the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-773cd104d6e8>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, ST, P, hd)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeep_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeep_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhd\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeep_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m180\u001b[0m\u001b[1;33m;\u001b[0m \u001b[1;31m# 0-2pi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mST\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mST\u001b[0m \u001b[1;31m# spiketimes (not train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 66049 is out of bounds for axis 0 with size 66049"
     ]
    }
   ],
   "source": [
    "numUnits = len(ST_all)\n",
    "llh_err_units = {}\n",
    "best_model_units = {}\n",
    "\n",
    "for u in range(numUnits):\n",
    "    print('unit ' + str(u))\n",
    "    ST = ST_all[u]\n",
    "    HD = HD_all[u]\n",
    "    RP = RP_all[u][0]\n",
    "    P = P_all[int(whichSess[u])]\n",
    "    t_raw = P[:,0]; dt = t_raw[1]-t_raw[0];\n",
    "\n",
    "    # initialize class instance\n",
    "    g = glm(ST,P,HD)\n",
    "\n",
    "    # prepare the data\n",
    "    posgrid_raw,bins = g.pos_map(nbins=10)\n",
    "    ebgrid_raw,bins = g.eb_map(nbins=10, rp=RP)\n",
    "    smooth_fr, raw_spktrn, filt, dt = g.conv_spktrain() # get spiketrain\n",
    "    posgrid,ebgrid,spiketrain = g.speed_threshold(posgrid_raw,ebgrid_raw,raw_spktrn)\n",
    "\n",
    "    # dictionaries with info about each model\n",
    "    stateDict = {\n",
    "        0: [posgrid,ebgrid],\n",
    "        1: posgrid,\n",
    "        2: ebgrid\n",
    "    }\n",
    "\n",
    "    labelDict = {\n",
    "        0: 'PE',\n",
    "        1: 'P',\n",
    "        2: 'E'\n",
    "    }\n",
    "\n",
    "    allModels = {}\n",
    "    numModels = 3\n",
    "\n",
    "    # get test/train indices (same for each model)\n",
    "    kfoldIdx, kfoldIdx_df = g.kfoldSplit(nfolds=10)\n",
    "\n",
    "    # iterate through all models of interest\n",
    "    for model in range(numModels):\n",
    "        # model = 0\n",
    "        mt = labelDict[model]\n",
    "\n",
    "#         print('processing model ' + str(model) + ' (' + mt + ') ...')\n",
    "\n",
    "        modelDict = {}\n",
    "\n",
    "        # get state matrix\n",
    "        stateIn = stateDict[model]\n",
    "        statemat, expr = g.squish_statemat(spiketrain, stateIn, modelType=mt)\n",
    "\n",
    "        # intialize output structures\n",
    "        modelType = labelDict[model]\n",
    "        _,nfolds=np.shape(kfoldIdx_df)\n",
    "        k_vec = np.arange(nfolds)\n",
    "        kres = {}\n",
    "        train_y = {}\n",
    "        train_x = {}\n",
    "        test_y = {}\n",
    "        test_x = {}\n",
    "        train_y_raw = {}\n",
    "        test_y_raw = {}\n",
    "\n",
    "        for foldnum in range(nfolds):\n",
    "            k_vec_train = np.delete(k_vec, np.where(k_vec == foldnum))\n",
    "            idx_test = kfoldIdx_df.loc[:,foldnum].to_numpy()\n",
    "            idx_train = []\n",
    "\n",
    "            # squeeze other nfolds-1 folds into one vector\n",
    "            for i,v in enumerate(k_vec_train):\n",
    "                nextRow = kfoldIdx_df.loc[:,v].to_numpy()\n",
    "                idx_train = np.append(idx_train,nextRow) \n",
    "            idx_train = idx_train.astype(int)\n",
    "\n",
    "            # train-test statemats\n",
    "            # @ note: is dropping nans here a problem???\n",
    "            df_test = statemat.loc[idx_test,:].dropna()\n",
    "            df_train = statemat.loc[idx_train,:].dropna()\n",
    "            y_test_raw = df_test['y'].to_numpy(dtype='int64')\n",
    "            y_train_raw = df_train['y'].to_numpy(dtype='int64')\n",
    "\n",
    "            # smooth firing rates\n",
    "            y_test, _, _, _ = g.conv_spktrain(defaultST=False,spikeIn=y_test_raw)\n",
    "            y_train, _, _, _ = g.conv_spktrain(defaultST=False,spikeIn=y_train_raw) \n",
    "\n",
    "            # put smoothed firing rates back into dataframe\n",
    "            df_test[df_test.columns[0]] = y_test\n",
    "            df_train[df_train.columns[0]] = y_train\n",
    "\n",
    "            # test/train arrays\n",
    "            X_test = df_test[df_test.columns[1:]].to_numpy(); \n",
    "            y_test = df_test[df_test.columns[0]].to_numpy()\n",
    "            X_train = df_train[df_train.columns[1:]].to_numpy()\n",
    "            y_train = df_train[df_train.columns[0]].to_numpy()\n",
    "\n",
    "            # set some initial parameters\n",
    "            M,n = np.shape(X_train)\n",
    "            w_0 = np.ones((n, ))*1e-3\n",
    "            b_0 = 1\n",
    "            # alpha = 0.001 (can't remember when we use this)\n",
    "\n",
    "            # get parameters & jacobian (1st order derivatives of loss fn)\n",
    "            data,param = g.getDataParam(X_train,y_train,w_0,b_0,modelType)\n",
    "\n",
    "            # not being used ** (depreciated right now)\n",
    "            # jac = self.grad(param,X_train,y_train)\n",
    "\n",
    "            # optimize loss function\n",
    "            res = g.bfgs(data,param)\n",
    "\n",
    "            # package outputs for each fold\n",
    "            kres[foldnum] = res\n",
    "            train_y[foldnum] = y_train; test_y[foldnum] = y_test\n",
    "            train_x[foldnum] = X_train; test_x[foldnum] = X_test\n",
    "            train_y_raw[foldnum] = y_train_raw; \n",
    "            test_y_raw[foldnum] = y_test_raw; \n",
    "\n",
    "            # optimize model parameters\n",
    "            kres, train_y, test_y, train_x, test_x, train_y_raw,test_y_raw, data, param = g.kfoldOptim(kfoldIdx_df,statemat,modelType=mt)\n",
    "\n",
    "            # check the model fit\n",
    "            testfit = g.get_testFit(mt,kres,train_y,test_y,train_x,test_x,train_y_raw,test_y_raw)\n",
    "\n",
    "        modelDict['kfoldIdx'] = kfoldIdx_df\n",
    "        modelDict['kres'] = kres\n",
    "        modelDict['train_y'] = train_y\n",
    "        modelDict['train_x'] = train_x\n",
    "        modelDict['test_y'] = test_y\n",
    "        modelDict['test_x'] = test_x\n",
    "        modelDict['train_y_raw'] = train_y_raw\n",
    "        modelDict['test_y_raw'] = test_y_raw\n",
    "        modelDict['testfit'] = testfit\n",
    "\n",
    "        # save in allModels dictionary\n",
    "        allModels[model] = modelDict\n",
    "\n",
    "    # model selection\n",
    "    llh, bestModel = g.findBestModel(allModels,labelDict)\n",
    "\n",
    "    # visualize results\n",
    "    llh_err = g.plot_llh(allModels,labelDict)\n",
    "\n",
    "    llh_err_units[u] = llh_err\n",
    "    best_model_units[u] = bestModel\n",
    "    \n",
    "    \n",
    "    \n",
    "    # save every 5th iteration\n",
    "    if np.mod(u, 5) == 0:\n",
    "        # Open a file and use dump()\n",
    "        with open('LLH.pkl', 'wb') as file:\n",
    "            # A new file will be created\n",
    "            pickle.dump(llh_err_units, file)\n",
    "            \n",
    "        with open('bestModel.pkl', 'wb') as file:\n",
    "            # A new file will be created\n",
    "            pickle.dump(best_model_units, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.42334574  0.35479703  7.72686492]\n",
      "[20.48367381  0.39840717  7.78719299]\n",
      "[21.77408548  1.75491483  9.07760465]\n",
      "[21.051732    1.01815325  8.35525117]\n",
      "[22.60899138  2.56396191  9.91251055]\n",
      "[21.17105524  1.12235044  8.47457441]\n",
      "[21.13917134  1.08739305  8.44269051]\n",
      "[20.99011938  1.09598808  8.29363856]\n",
      "[20.86147347  0.80317502  8.16499264]\n"
     ]
    }
   ],
   "source": [
    "for unit in range(9):\n",
    "    llh_stack = np.hstack((np.sort(llh_err_units[unit][0],0), np.sort(llh_err_units[unit][1],0), np.sort(llh_err_units[unit][2],0)))\n",
    "    print(np.median(llh_stack,0))\n",
    "    del llh_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.49715352,  1.44963803,  8.8006727 ])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(llh_stack,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0:       fun: 21.065804263492517\n",
       "  hess_inv: <111x111 LbfgsInvHessProduct with dtype=float64>\n",
       "       jac: array([ 1.55556974,  0.50830425,  0.52114967,  0.0916469 ,  0.15692069,\n",
       "         0.11128232,  0.09136419,  0.11561509,  0.28600881,  0.        ,\n",
       "         1.09396441,  0.66758715,  0.16082064,  0.18399554,  0.14283697,\n",
       "         0.08050131,  0.14363561,  0.10855367,  0.44553177,  0.        ,\n",
       "         0.90855638,  0.30859421,  0.14069227,  0.18070574,  0.11966728,\n",
       "         0.13188149,  0.13548272,  0.15292133,  0.57584802,  0.        ,\n",
       "         0.35299543,  0.17191081,  0.21491993,  0.11946501,  0.11550978,\n",
       "         0.12900629,  0.10492661,  0.16411638,  0.38885092,  0.        ,\n",
       "         0.30445033,  0.11962277,  0.20644863,  0.12128248,  0.15344604,\n",
       "         0.08708821,  0.21822578,  0.18289562,  0.30310233,  0.        ,\n",
       "         0.17555442,  0.10272341,  0.11294909,  0.08257619,  0.06005158,\n",
       "         0.14905897,  0.07806416,  0.17523008,  0.18052069,  0.        ,\n",
       "         0.16677577,  0.13683181,  0.06946548,  0.06863037,  0.16986449,\n",
       "         0.0969404 ,  0.07720917,  0.04236333,  0.29895746,  0.        ,\n",
       "         0.32736056,  0.1675161 ,  0.16590518,  0.14850697,  0.11092433,\n",
       "         0.12011369,  0.08137884,  0.08217002,  0.30586642,  0.        ,\n",
       "         0.74791005,  0.56433985,  0.8014188 ,  0.24579596,  0.24910656,\n",
       "         0.15597957,  0.21519249,  0.23463774,  0.49717289,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        13.76998762,  1.49350089,  1.10260351,  0.62045517,  0.56063364,\n",
       "         0.55537881,  0.5159311 ,  0.60885062,  0.8156231 ,  0.        ,\n",
       "        20.04296445])\n",
       "   message: b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
       "      nfev: 21\n",
       "       nit: 0\n",
       "    status: 2\n",
       "   success: False\n",
       "         x: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1.]), 1:       fun: 21.024859436010495\n",
       "  hess_inv: <111x111 LbfgsInvHessProduct with dtype=float64>\n",
       "       jac: array([ 1.53842197,  0.50744637,  0.53530468,  0.09422054,  0.16035221,\n",
       "         0.10613504,  0.07506447,  0.10789418,  0.27013804,  0.        ,\n",
       "         1.2198537 ,  0.6354272 ,  0.16082064,  0.1857113 ,  0.16857336,\n",
       "         0.06751798,  0.14964077,  0.1282849 ,  0.41035581,  0.        ,\n",
       "         0.70823165,  0.27556585,  0.12739513,  0.16783755,  0.135967  ,\n",
       "         0.11643966,  0.15435608,  0.13501045,  0.57859133,  0.        ,\n",
       "         0.22067204,  0.1117441 ,  0.18618096,  0.07395577,  0.12488348,\n",
       "         0.10155414,  0.10363979,  0.15864476,  0.29653506,  0.        ,\n",
       "         0.20482404,  0.10246518,  0.2791592 ,  0.13801114,  0.1615959 ,\n",
       "         0.1342716 ,  0.26455128,  0.19619276,  0.30567597,  0.        ,\n",
       "         0.1687931 ,  0.11087326,  0.1549852 ,  0.08000255,  0.08321433,\n",
       "         0.15849565,  0.07549052,  0.15978825,  0.18266539,  0.        ,\n",
       "         0.18950958,  0.12953983,  0.07118124,  0.06863037,  0.16815856,\n",
       "         0.08964843,  0.07463553,  0.04836849,  0.37530875,  0.        ,\n",
       "         0.32307116,  0.16236882,  0.16461836,  0.14421757,  0.09042999,\n",
       "         0.11496641,  0.0809499 ,  0.07745169,  0.34790253,  0.        ,\n",
       "         0.67284558,  0.54503755,  0.871336  ,  0.29769768,  0.25425383,\n",
       "         0.16412943,  0.33776469,  0.26208989,  0.65920754,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        13.60152647,  1.49864958,  1.03895683,  0.68798884,  0.62920738,\n",
       "         0.56411072,  0.52193915,  0.59671939,  0.90564431,  0.        ,\n",
       "        20.04474266])\n",
       "   message: b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
       "      nfev: 21\n",
       "       nit: 0\n",
       "    status: 2\n",
       "   success: False\n",
       "         x: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1.]), 2:       fun: 21.012816839692267\n",
       "  hess_inv: <111x111 LbfgsInvHessProduct with dtype=float64>\n",
       "       jac: array([ 1.39740536,  0.39827591,  0.49326858,  0.08049447,  0.15906539,\n",
       "         0.11319457,  0.09522464,  0.10789418,  0.21823631,  0.        ,\n",
       "         1.27076826,  0.68851114,  0.11235044,  0.152254  ,  0.10037192,\n",
       "         0.08009491,  0.13406344,  0.13214536,  0.35823435,  0.        ,\n",
       "         0.9973371 ,  0.28216059,  0.13039771,  0.16869543,  0.11923834,\n",
       "         0.13702877,  0.16550851,  0.13243681,  0.58829017,  0.        ,\n",
       "         0.32425646,  0.15823618,  0.20762796,  0.11599187,  0.13903849,\n",
       "         0.14316131,  0.11522117,  0.1718373 ,  0.35668043,  0.        ,\n",
       "         0.28815062,  0.11833595,  0.31637236,  0.14701887,  0.16760105,\n",
       "         0.1342716 ,  0.26455128,  0.18504032,  0.28465792,  0.        ,\n",
       "         0.13737878,  0.1177363 ,  0.14983792,  0.07313951,  0.08321433,\n",
       "         0.16492975,  0.07591946,  0.16965387,  0.20711496,  0.        ,\n",
       "         0.16677577,  0.09222207,  0.0686076 ,  0.04546762,  0.15013326,\n",
       "         0.07892493,  0.06905931,  0.04836849,  0.37530875,  0.        ,\n",
       "         0.31363448,  0.1675161 ,  0.15260804,  0.13864135,  0.09290886,\n",
       "         0.11196384,  0.0916734 ,  0.07745169,  0.34575783,  0.        ,\n",
       "         0.60303299,  0.54718225,  0.79369788,  0.29093635,  0.23632308,\n",
       "         0.14013157,  0.33915611,  0.26037413,  0.61748524,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        13.43124447,  1.49996065,  1.10422449,  0.71825346,  0.64341902,\n",
       "         0.51384293,  0.59014993,  0.62643716,  0.91773356,  0.        ,\n",
       "        20.04526567])\n",
       "   message: b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
       "      nfev: 21\n",
       "       nit: 0\n",
       "    status: 2\n",
       "   success: False\n",
       "         x: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1.]), 3:       fun: 20.935744223255508\n",
       "  hess_inv: <111x111 LbfgsInvHessProduct with dtype=float64>\n",
       "       jac: array([ 1.65123315,  0.53704322,  0.4829968 ,  0.07019991,  0.08235645,\n",
       "         0.11518833,  0.08964843,  0.10960993,  0.29244291,  0.        ,\n",
       "         1.03206158,  0.70738449,  0.10720316,  0.18828494,  0.15699198,\n",
       "         0.08695819,  0.15011822,  0.13429006,  0.40821111,  0.        ,\n",
       "         1.02696655,  0.3103918 ,  0.13425817,  0.19657652,  0.12653032,\n",
       "         0.1285318 ,  0.15178244,  0.1290053 ,  0.57617525,  0.        ,\n",
       "         0.3053831 ,  0.17071877,  0.20891478,  0.1065552 ,  0.14247001,\n",
       "         0.13115099,  0.11693693,  0.18899489,  0.31965397,  0.        ,\n",
       "         0.28997098,  0.11619125,  0.33415467,  0.12471689,  0.1560295 ,\n",
       "         0.11539824,  0.24396217,  0.18830459,  0.23119857,  0.        ,\n",
       "         0.15024697,  0.10486811,  0.14812216,  0.07142375,  0.08149857,\n",
       "         0.09286785,  0.06648278,  0.16321977,  0.16507886,  0.        ,\n",
       "         0.16291531,  0.13254241,  0.06388926,  0.06863037,  0.14937638,\n",
       "         0.07806705,  0.05619112,  0.04193439,  0.31866762,  0.        ,\n",
       "         0.28446657,  0.15507684,  0.15818426,  0.12332547,  0.09341411,\n",
       "         0.09309048,  0.07708944,  0.06072303,  0.28399049,  0.        ,\n",
       "         0.73589974,  0.50245737,  0.86157498,  0.29726874,  0.21135985,\n",
       "         0.17270822,  0.3540644 ,  0.27195551,  0.52279414,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        13.78519358,  1.51878564,  0.95697316,  0.61112021,  0.58081265,\n",
       "         0.57426511,  0.58889108,  0.57670291,  0.85586855,  0.        ,\n",
       "        20.04861289])\n",
       "   message: b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
       "      nfev: 21\n",
       "       nit: 0\n",
       "    status: 2\n",
       "   success: False\n",
       "         x: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1.]), 4:       fun: 21.03696298731512\n",
       "  hess_inv: <111x111 LbfgsInvHessProduct with dtype=float64>\n",
       "       jac: array([ 1.41927744,  0.39208667,  0.53533898,  0.09422658,  0.13376651,\n",
       "         0.11772396,  0.0853645 ,  0.12806255,  0.31047827,  0.        ,\n",
       "         1.19053251,  0.58517413,  0.16083094,  0.19087081,  0.16429448,\n",
       "         0.05734224,  0.15522693,  0.14330698,  0.48073274,  0.        ,\n",
       "         1.04395905,  0.31719333,  0.16172068,  0.19658911,  0.1308281 ,\n",
       "         0.13879898,  0.15479493,  0.14134901,  0.5980895 ,  0.        ,\n",
       "         0.36417119,  0.18865155,  0.20420952,  0.11620852,  0.14354629,\n",
       "         0.13073043,  0.12611639,  0.17699591,  0.38201235,  0.        ,\n",
       "         0.30489881,  0.12477804,  0.34084377,  0.14702829,  0.16761179,\n",
       "         0.11164665,  0.14155922,  0.108696  ,  0.31985148,  0.        ,\n",
       "         0.17728154,  0.11774385,  0.14684475,  0.08086561,  0.0682058 ,\n",
       "         0.13297987,  0.08321677,  0.11818866,  0.17845041,  0.        ,\n",
       "         0.19424037,  0.10381009,  0.04973743,  0.05876852,  0.15057185,\n",
       "         0.08750933,  0.06048439,  0.03936328,  0.33536532,  0.        ,\n",
       "         0.33134974,  0.16752683,  0.16119717,  0.14851648,  0.1100735 ,\n",
       "         0.10939721,  0.0873896 ,  0.0873229 ,  0.34020341,  0.        ,\n",
       "         0.52852249,  0.54635938,  0.64371478,  0.27422815,  0.18823193,\n",
       "         0.14813023,  0.32963595,  0.21492028,  0.61635305,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        13.66968102,  1.53778961,  1.07621539,  0.67593632,  0.62458569,\n",
       "         0.52967635,  0.51982486,  0.57719161,  0.83331618,  0.        ,\n",
       "        20.04421701])\n",
       "   message: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
       "      nfev: 21\n",
       "       nit: 1\n",
       "    status: 0\n",
       "   success: True\n",
       "         x: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1.]), 5:       fun: 21.015284925094495\n",
       "  hess_inv: <111x111 LbfgsInvHessProduct with dtype=float64>\n",
       "       jac: array([ 1.54924472,  0.41760496,  0.5246148 ,  0.07288282,  0.13848515,\n",
       "         0.11557912,  0.09222797,  0.12806255,  0.30275686,  0.        ,\n",
       "         1.31644951,  0.53895026,  0.14024051,  0.11794637,  0.16129171,\n",
       "         0.08200259,  0.13306686,  0.11220158,  0.41820812,  0.        ,\n",
       "         0.87760156,  0.27386763,  0.1548572 ,  0.16195026,  0.12139775,\n",
       "         0.12298624,  0.13463347,  0.1374883 ,  0.5051082 ,  0.        ,\n",
       "         0.32794096,  0.17069791,  0.19734604,  0.12157588,  0.14000012,\n",
       "         0.11914831,  0.11136784,  0.17234026,  0.37000127,  0.        ,\n",
       "         0.22221272,  0.1063839 ,  0.33955687,  0.1088502 ,  0.14616343,\n",
       "         0.11486153,  0.18992792,  0.18290734,  0.28724996,  0.        ,\n",
       "         0.12773054,  0.11397792,  0.13418494,  0.06166669,  0.08321966,\n",
       "         0.16193754,  0.07806916,  0.16237229,  0.20069372,  0.        ,\n",
       "         0.1509965 ,  0.12440052,  0.07461754,  0.06863477,  0.16987538,\n",
       "         0.09694661,  0.07721412,  0.04289673,  0.26234609,  0.        ,\n",
       "         0.27592823,  0.12806184,  0.1607682 ,  0.14851648,  0.10707073,\n",
       "         0.11336252,  0.08996629,  0.0873229 ,  0.30432792,  0.        ,\n",
       "         0.7411991 ,  0.53263242,  0.85552003,  0.29642985,  0.25384116,\n",
       "         0.18387244,  0.36095056,  0.27411777,  0.65924978,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        13.72405876,  1.5002403 ,  1.08853347,  0.61685928,  0.60271818,\n",
       "         0.55633531,  0.54785397,  0.60380501,  0.8047542 ,  0.        ,\n",
       "        20.04515848])\n",
       "   message: b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
       "      nfev: 21\n",
       "       nit: 0\n",
       "    status: 2\n",
       "   success: False\n",
       "         x: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1.]), 6:       fun: 21.038745118837102\n",
       "  hess_inv: <111x111 LbfgsInvHessProduct with dtype=float64>\n",
       "       jac: array([ 1.42581617,  0.5063401 ,  0.47725222,  0.08199808,  0.15747353,\n",
       "         0.11897305,  0.09351515,  0.11016962,  0.26739464,  0.        ,\n",
       "         1.18615986,  0.68710303,  0.15161495,  0.16806487,  0.15880725,\n",
       "         0.0862172 ,  0.12294373,  0.10197125,  0.41688989,  0.        ,\n",
       "         0.92194828,  0.24871437,  0.14911875,  0.14480079,  0.11962518,\n",
       "         0.09581477,  0.13357447,  0.13206275,  0.61764405,  0.        ,\n",
       "         0.33612737,  0.1738795 ,  0.08593284,  0.10085113,  0.1298026 ,\n",
       "         0.11752579,  0.12747483,  0.18391705,  0.37133918,  0.        ,\n",
       "         0.27918651,  0.10146817,  0.30479541,  0.14185212,  0.16459223,\n",
       "         0.13186112,  0.25980199,  0.17582108,  0.30734946,  0.        ,\n",
       "         0.1614506 ,  0.08363084,  0.11352919,  0.07435393,  0.08172044,\n",
       "         0.16196888,  0.0817176 ,  0.16492324,  0.17517374,  0.        ,\n",
       "         0.19074109,  0.13437537,  0.07158833,  0.0577098 ,  0.16681504,\n",
       "         0.0952001 ,  0.07582309,  0.04750017,  0.35551269,  0.        ,\n",
       "         0.33369964,  0.15734774,  0.14523475,  0.10055252,  0.06554533,\n",
       "         0.10489896,  0.07739047,  0.07564001,  0.2785737 ,  0.        ,\n",
       "         0.73785329,  0.57737683,  0.83084041,  0.27444548,  0.25011064,\n",
       "         0.17761127,  0.31621507,  0.23927433,  0.63220866,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        13.74968808,  1.41614043,  1.08355868,  0.67166003,  0.58721682,\n",
       "         0.50085985,  0.55278667,  0.58991966,  0.89230941,  0.        ,\n",
       "        20.04413962])\n",
       "   message: b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
       "      nfev: 21\n",
       "       nit: 0\n",
       "    status: 2\n",
       "   success: False\n",
       "         x: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1.]), 7:       fun: 20.926184124923296\n",
       "  hess_inv: <111x111 LbfgsInvHessProduct with dtype=float64>\n",
       "       jac: array([ 1.61606347,  0.51685723,  0.43482909,  0.08859114,  0.14417623,\n",
       "         0.05064836,  0.07107171,  0.08537016,  0.21657946,  0.        ,\n",
       "         1.19271411,  0.66578047,  0.15725177,  0.17871136,  0.14550775,\n",
       "         0.08607459,  0.1525989 ,  0.14049251,  0.41098021,  0.        ,\n",
       "         0.86457303,  0.21970608,  0.14634884,  0.19398984,  0.12783816,\n",
       "         0.13644892,  0.15176448,  0.14329188,  0.53267717,  0.        ,\n",
       "         0.33515085,  0.18578762,  0.21155361,  0.11288006,  0.13673713,\n",
       "         0.13363001,  0.11927366,  0.18277175,  0.36736061,  0.        ,\n",
       "         0.2825096 ,  0.11265499,  0.32111296,  0.12395469,  0.07180061,\n",
       "         0.11860619,  0.25937225,  0.19235196,  0.29170155,  0.        ,\n",
       "         0.16917386,  0.09324521,  0.14522242,  0.06634318,  0.06560465,\n",
       "         0.15286958,  0.06854562,  0.17095858,  0.17656616,  0.        ,\n",
       "         0.1786504 ,  0.13331202,  0.06392248,  0.06728682,  0.15182013,\n",
       "         0.07149225,  0.06854845,  0.03144098,  0.35113976,  0.        ,\n",
       "         0.31590543,  0.13625581,  0.13499419,  0.10018109,  0.1087528 ,\n",
       "         0.094099  ,  0.08146789,  0.07425327,  0.32216735,  0.        ,\n",
       "         0.72842708,  0.53551698,  0.85138174,  0.28976704,  0.23035198,\n",
       "         0.16328343,  0.32467208,  0.26873424,  0.63252712,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        13.80765363,  1.44879063,  1.07091107,  0.6127573 ,  0.55177936,\n",
       "         0.52822509,  0.54294121,  0.598193  ,  0.88777679,  0.        ,\n",
       "        20.04902808])\n",
       "   message: b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
       "      nfev: 21\n",
       "       nit: 0\n",
       "    status: 2\n",
       "   success: False\n",
       "         x: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1.]), 8:       fun: 20.909654545113906\n",
       "  hess_inv: <111x111 LbfgsInvHessProduct with dtype=float64>\n",
       "       jac: array([ 1.43376623,  0.49961498,  0.52482521,  0.09237602,  0.15721305,\n",
       "         0.11877626,  0.09756589,  0.12554747,  0.30438065,  0.        ,\n",
       "         1.24896144,  0.7062143 ,  0.15097481,  0.17577719,  0.16737597,\n",
       "         0.08607459,  0.16058921,  0.14049251,  0.47129141,  0.        ,\n",
       "         0.99485936,  0.29918863,  0.14046124,  0.19735418,  0.13078196,\n",
       "         0.13476675,  0.16437113,  0.15287144,  0.63764955,  0.        ,\n",
       "         0.34860822,  0.1756946 ,  0.21155361,  0.10909517,  0.14220418,\n",
       "         0.13194784,  0.13020777,  0.19244423,  0.390911  ,  0.        ,\n",
       "         0.28671502,  0.11391662,  0.33583196,  0.13362717,  0.14507758,\n",
       "         0.11902673,  0.25937225,  0.15534421,  0.30852326,  0.        ,\n",
       "         0.17379983,  0.10197406,  0.14017591,  0.0767542 ,  0.07107171,\n",
       "         0.15371067,  0.05971705,  0.13962016,  0.20306035,  0.        ,\n",
       "         0.19042559,  0.12700388,  0.06432069,  0.06728682,  0.07474794,\n",
       "         0.0769593 ,  0.06854845,  0.0474216 ,  0.35130829,  0.        ,\n",
       "         0.20635872,  0.11629484,  0.15676972,  0.1455997 ,  0.10286521,\n",
       "         0.10472545,  0.08041136,  0.08560792,  0.26804898,  0.        ,\n",
       "         0.50909552,  0.47923347,  0.57116241,  0.28219727,  0.24801477,\n",
       "         0.17311206,  0.33073185,  0.21290462,  0.58448271,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        13.47785709,  1.55993144,  1.09187142,  0.60376583,  0.63208467,\n",
       "         0.59370984,  0.60297625,  0.6238461 ,  0.8637033 ,  0.        ,\n",
       "        20.04974595])\n",
       "   message: b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
       "      nfev: 21\n",
       "       nit: 0\n",
       "    status: 2\n",
       "   success: False\n",
       "         x: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1.]), 9:       fun: 21.011193392514542\n",
       "  hess_inv: <111x111 LbfgsInvHessProduct with dtype=float64>\n",
       "       jac: array([ 1.67053174,  0.51012855,  0.25524705,  0.07475404,  0.14165298,\n",
       "         0.11499138,  0.09756589,  0.12554747,  0.30438065,  0.        ,\n",
       "         1.3196126 ,  0.57956922,  0.13370138,  0.16273074,  0.15812403,\n",
       "         0.07093506,  0.16058921,  0.13773288,  0.47129141,  0.        ,\n",
       "         1.02303572,  0.29834755,  0.15854457,  0.18894333,  0.13961336,\n",
       "         0.13855163,  0.15086262,  0.13585523,  0.59685692,  0.        ,\n",
       "         0.33557139,  0.18410545,  0.19977841,  0.10909517,  0.14977395,\n",
       "         0.14035869,  0.13020777,  0.16048299,  0.36609899,  0.        ,\n",
       "         0.27451929,  0.12527127,  0.32195405,  0.1265805 ,  0.16389944,\n",
       "         0.13164301,  0.25937225,  0.18436165,  0.31567248,  0.        ,\n",
       "         0.14141804,  0.1053384 ,  0.15363328,  0.07044606,  0.06560465,\n",
       "         0.14361764,  0.07611538,  0.15623959,  0.18035105,  0.        ,\n",
       "         0.14332482,  0.10765892,  0.06894666,  0.04205426,  0.16653912,\n",
       "         0.09420155,  0.06181977,  0.04237509,  0.32685084,  0.        ,\n",
       "         0.32347519,  0.13900414,  0.08275421,  0.12919854,  0.1087528 ,\n",
       "         0.10640762,  0.07128431,  0.07191279,  0.31123324,  0.        ,\n",
       "         0.70509213,  0.43342698,  0.82115785,  0.2292089 ,  0.21395082,\n",
       "         0.16259849,  0.31485378,  0.20974539,  0.46629989,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        13.68321019,  1.56761032,  1.07373686,  0.66141917,  0.58708661,\n",
       "         0.56427186,  0.59204214,  0.58327962,  0.73267941,  0.        ,\n",
       "        20.04533617])\n",
       "   message: b'ABNORMAL_TERMINATION_IN_LNSRCH'\n",
       "      nfev: 21\n",
       "       nit: 0\n",
       "    status: 2\n",
       "   success: False\n",
       "         x: array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1.])}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# allModels[0]['train_y'][0]\n",
    "allModels[0]['kres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
