{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM-eb \n",
    "J. Carpenter and B. Dunn \\\n",
    "November 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "import scipy.sparse as sps\n",
    "import scipy.stats as stats\n",
    "from pyglmnet import GLM, simulate_glm\n",
    "import scipy as sp\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization parameters\n",
    "plt.rcParams['figure.figsize'] = (10,4)\n",
    "plt.rc('axes', labelsize=12); plt.rc('axes', titlesize=14)\n",
    "plt.style.use('ggplot'); plt.rc('font', size=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load & format data\n",
    "filepath = 'sampleData.mat'\n",
    "mat = scipy.io.loadmat(filepath)\n",
    "ST = mat['ST']; P = mat['P']; hd = mat['hd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class glm:\n",
    "    def __init__(self, ST, P, hd):\n",
    "        self.ST = ST\n",
    "        self.P = P\n",
    "        self.x = P[:,1]\n",
    "        self.y = P[:,2]\n",
    "        self.t = P[:,0]\n",
    "        self.hd = (hd[:,0]*np.pi)/180; # 0-2pi\n",
    "        self.dt = np.round(statistics.mode(np.diff(P[:,0])),2);\n",
    "        \n",
    "    def get_size(self):\n",
    "        '''get size of recording box'''\n",
    "        boxsz = np.max([np.max(self.x), np.max(self.y)]);\n",
    "        return boxsz\n",
    "    \n",
    "    def pos_map(self, nbins=10):\n",
    "        '''design matrix for position variables'''\n",
    "        boxsz = self.get_size()\n",
    "        bins = np.arange(boxsz/nbins/2, boxsz-boxsz/nbins/2, boxsz/nbins)\n",
    "        posgrid = np.zeros((len(self.x), nbins**2))\n",
    "        for idx,val in enumerate(self.x):\n",
    "            xvec = np.abs(self.x[idx]-bins); yvec = np.abs(self.y[idx]-bins);\n",
    "            min_x = np.min(xvec)\n",
    "            min_y = np.min(yvec)\n",
    "            idx_x = np.where(xvec == min_x); idx_x = idx_x[0][0];\n",
    "            idx_y = np.where(yvec == min_y); idx_y = idx_y[0][0];\n",
    "            bin_idx = np.ravel_multi_index((idx_y,idx_x), dims=(nbins,nbins), order='C') # a11=0, a12=1, a13=2;\n",
    "            posgrid[idx, bin_idx] = 1;\n",
    "        return posgrid, bins\n",
    "    \n",
    "    def eb_map(self, nbins=10, rp=[75,75]):\n",
    "        '''design matrix for egocentric variables'''\n",
    "        refx = rp[0]; refy = rp[1];\n",
    "        allo = np.arctan2(refy-self.y, refx-self.x) + (np.pi/2); # add 90 deg\n",
    "        allo[allo<0] = allo[allo<0]+2*np.pi;\n",
    "        ego = allo - self.hd; # shift from 0-2pi\n",
    "        egogrid = np.zeros((len(P),nbins));\n",
    "        bins = np.arange(2*np.pi/nbins/2, 2*np.pi-2*np.pi/nbins/2, 2*np.pi/nbins) # 10 bin ctrs\n",
    "        for idx,val in enumerate(P):\n",
    "            evec = np.abs(ego[idx]-bins)\n",
    "            min_e = np.min(evec)\n",
    "            idx_e = np.where(evec == min_e)\n",
    "            egogrid[idx, idx_e] = 1;\n",
    "        return egogrid, bins\n",
    "    \n",
    "    def conv_spktrain(self):\n",
    "        '''get smoothed spiketrain from spiketimes'''\n",
    "        # filter the spiketrain\n",
    "        t = self.P[:,0];\n",
    "        boolean_spk = np.logical_and(t[0] <= self.ST, self.ST <= t[-1])\n",
    "        spikes = self.ST[boolean_spk == True]\n",
    "        edgesT = np.linspace(t[0], t[-1], len(t)+1)\n",
    "        binnedSpikes, timeEdges = np.histogram(spikes, edgesT)\n",
    "        # convolve w/ gaussian membership fn\n",
    "        Xx = np.linspace(-4,4,9); sigma = 2; c = 0;\n",
    "        filt = np.exp((-(Xx-c)**2)/(2*(sigma**2)))\n",
    "        dt = self.P[1,0]-self.P[0,0];\n",
    "        fr = binnedSpikes/dt # rate (hz)\n",
    "        smooth_fr = np.convolve(binnedSpikes, filt, mode='full')\n",
    "        return smooth_fr, binnedSpikes, filt, dt\n",
    "    \n",
    "    def get_speed(self):\n",
    "        '''get speed of the animal (cm*s^-2)'''\n",
    "        t=self.P[:,0]; x=self.P[:,1]; y=self.P[:,2];\n",
    "        ntime = len(t); v = np.zeros((ntime,1));\n",
    "        for idx in range(1,ntime-1):\n",
    "            v[idx,0] = np.sqrt((x[idx+1]-x[idx-1])**2 + (y[idx+1]-y[idx-1])**2)/(t[idx+1]-t[idx-1])    \n",
    "        v[0,0] = v[1,0]; v[-1,0] = v[-2,0] # pad the array\n",
    "        return v\n",
    "    \n",
    "    def speed_threshold(self,posgrid,ebgrid,spiketrain):\n",
    "        v = self.get_speed()\n",
    "        maxspeed=50; minspeed=4\n",
    "        inbounds = np.logical_and((v<=maxspeed), (v>=minspeed))\n",
    "        inbounds = np.where(inbounds==True); inbounds = inbounds[0]\n",
    "        posgrid = posgrid[inbounds,:]\n",
    "        ebgrid = ebgrid[inbounds,:]\n",
    "        spiketrain = spiketrain[inbounds]\n",
    "        return posgrid, ebgrid, spiketrain\n",
    "    \n",
    "    def squish_statemat(self, stateIn, modelType='PE'):\n",
    "        '''squish state matrix for 2-variable model (P+EB)'''\n",
    "        if modelType == 'PE':\n",
    "            posgrid = stateIn[0]; ebgrid = stateIn[1]\n",
    "            ntime,nbins_eb = np.shape(ebgrid)\n",
    "            _,nbins_p = np.shape(posgrid)\n",
    "            A = np.zeros((ntime, nbins_p+nbins_eb)) #P+EB\n",
    "            A[:,0:nbins_p] = posgrid; A[:,nbins_p:] = ebgrid\n",
    "            df=pd.DataFrame(A)\n",
    "            mask = np.random.rand(len(df)) < 0.8\n",
    "            df_train = df[mask]; df_test = df[~mask]\n",
    "            # name columns & get expression\n",
    "            colnames = [];\n",
    "            expr = 'y ~ '\n",
    "            for i in range(nbins_p):\n",
    "                val = str(i);\n",
    "                expr = expr + 'P' + val + ' + '\n",
    "                colnames.append('P' + val)\n",
    "            for i in range(nbins_eb-1):\n",
    "                val = str(i);\n",
    "                expr = expr + 'E' + val + ' + '\n",
    "                colnames.append('E' + val)\n",
    "            expr = expr + 'E9'\n",
    "            colnames.append('E9')\n",
    "            df.columns = colnames\n",
    "        elif modelType == 'P':\n",
    "            ntime,nbins = np.shape(stateIn)\n",
    "            df = pd.DataFrame(stateIn)\n",
    "            mask = np.random.rand(len(df)) < 0.8\n",
    "            df_train = df[mask]; df_test = df[~mask]\n",
    "            colnames = [];\n",
    "            expr = 'y ~ '\n",
    "            for i in range(nbins-1):\n",
    "                val = str(i);\n",
    "                expr = expr + 'P' + val + ' + '\n",
    "                colnames.append('P' + val)\n",
    "            expr = expr + 'P99'\n",
    "            colnames.append('P99')\n",
    "            df.columns = colnames\n",
    "        elif modelType == 'E':\n",
    "            ntime,nbins = np.shape(stateIn)\n",
    "            df = pd.DataFrame(stateIn)\n",
    "            mask = np.random.rand(len(df)) < 0.8\n",
    "            df_train = df[mask]; df_test = df[~mask]\n",
    "            colnames = [];\n",
    "            expr = 'y ~ '\n",
    "            for i in range(nbins-1):\n",
    "                val = str(i);\n",
    "                expr = expr + 'E' + val + ' + '\n",
    "                colnames.append('E' + val)\n",
    "            expr = expr + 'E9'\n",
    "            colnames.append('E9')\n",
    "            df.columns = colnames\n",
    "        else:\n",
    "            print('Error: model type must be \"P\", \"E\", or \"PE\"')\n",
    "        return df,expr\n",
    "    \n",
    "    def kfoldSplit(self,nfolds=10):\n",
    "        '''train-test split for k-fold xval\n",
    "            each section is ~1 min'''\n",
    "        _, spiketrain, _, dt = self.conv_spktrain()\n",
    "        # calculate number of chunks given session length\n",
    "        nmins = (len(spiketrain)*dt)/60\n",
    "        nchunks = int(round(nmins/nfolds))\n",
    "        nsections = int(nchunks*nfolds)\n",
    "        # grab indices for k-fold splitting\n",
    "        kfoldIdx = {}\n",
    "        howLong = np.zeros(nfolds)\n",
    "        edges = np.round(np.linspace(1,len(spiketrain)+1,nsections+1))\n",
    "        for k in range(nfolds):\n",
    "            test_ind = np.floor(np.linspace(int(edges[k]),\n",
    "                    (int(edges[k+1])-1),\n",
    "                    (int(edges[k+1])-1)-int(edges[k])))\n",
    "            for s in range(1,nchunks):\n",
    "                ind = np.floor(np.linspace(int(edges[k+s*nfolds]),\n",
    "                                  (int(edges[k+s*nfolds+1])-1),\n",
    "                                  (int(edges[k+s*nfolds+1]))-int(edges[k+s*nfolds])))\n",
    "                np.append(test_ind,ind)\n",
    "            kfoldIdx[k] = test_ind\n",
    "            howLong[k] = len(test_ind); del test_ind;\n",
    "        minArrLen = int(np.min(howLong));\n",
    "        for k in range(nfolds):\n",
    "            kfoldIdx[k] = kfoldIdx[k][0:minArrLen] # adjust arr. len (w/in .02 s)\n",
    "        kfoldIdx_df = pd.DataFrame.from_dict(kfoldIdx)\n",
    "        kfoldIdx_df = kfoldIdx_df.astype(int) # for idxing purposes\n",
    "        return kfoldIdx, kfoldIdx_df\n",
    "    \n",
    "    ####################### DEPRECIATE #######################\n",
    "#     def test_train(self,df,expr,spiketrain):\n",
    "#         df.insert(loc=0, column='y', value=spiketrain, allow_duplicates=False)\n",
    "#         mask = np.random.rand(len(df)) < 0.8\n",
    "#         df_train = df[mask]\n",
    "#         df_test = df[~mask]\n",
    "#         # split into test and train, and booty\n",
    "#         y_train, X_train = dmatrices(expr, df_train, return_type='dataframe')\n",
    "#         y_test, X_test = dmatrices(expr, df_test, return_type='dataframe')\n",
    "# #         print('Training data set length='+str(len(df_train)))\n",
    "# #         print('Testing data set length='+str(len(df_test)))\n",
    "#         return y_train, X_train, y_test, X_test\n",
    "    \n",
    "#     def test_train_arr(self,df,expr,spiketrain):\n",
    "#         '''return training set as array (not df)'''\n",
    "#         y_train, X_train, y_test, X_test = self.test_train(df,expr,spiketrain)\n",
    "#         y_train_arr = y_train.to_numpy(); y_test_arr = y_test.to_numpy()\n",
    "#         X_test_arr = X_test.to_numpy(); X_train_arr = X_train.to_numpy()\n",
    "#         X_train_arr = X_train_arr[:,1:];  X_test_arr = X_test_arr[:,1:]\n",
    "#         return y_train_arr, X_train_arr, y_test_arr, X_test_arr\n",
    "    \n",
    "    ####################### DEPRECIATE #######################\n",
    "    \n",
    "    def init_params(self,whichVars={'P', 'E'}):\n",
    "        if whichVars == {'P', 'E'}: init_param = 1e-3*np.random.randn(110, 1);\n",
    "        if whichVars == {'P'}: init_param = 1e-3*np.random.randn(100, 1);\n",
    "        if whichVars == {'E'}: init_param = 1e-3*np.random.randn(10, 1);\n",
    "        return init_param\n",
    "    \n",
    "    def getDataParam(self,x,y,w,b):\n",
    "        '''put param & data in a dictionary'''\n",
    "        param = np.append(b,w)\n",
    "        data =  (x, y)\n",
    "        return data,param\n",
    "    \n",
    "    def get_rate(self,x,w,b):\n",
    "        '''conditional intensity function'''\n",
    "        y_hat = np.exp(x @ w + b)\n",
    "        return y_hat\n",
    "    \n",
    "    def loss(self,param,x,y):\n",
    "        '''objective function'''\n",
    "        y_hat = np.exp(x @ param[1:] + param[0])\n",
    "        error = (y_hat - np.log(y_hat) * y).mean() #negative log likelihood for possom where yhat is lambda \n",
    "        \n",
    "        #der= deriv\n",
    "        return error\n",
    "    \n",
    "    def grad(self,x, y, w, b):\n",
    "        '''compute the gradient of the loss fn'''\n",
    "        M, n = x.shape\n",
    "        y_hat = np.exp(x @ w + b)\n",
    "        dw = (x.T @ (y_hat - y)) / M\n",
    "        db = (y_hat - y).mean() # this we dont really need if the bias term is included as a column of 1's\n",
    "        return dw, db\n",
    "\n",
    "    def gradient_descent(self,x, y, w_0, b_0, alpha, num_iter):\n",
    "        '''minimize loss function w/ gradient descent'''\n",
    "        w, b = w_0.copy(), b_0\n",
    "        hist = np.zeros(num_iter)\n",
    "        M, n = x.shape\n",
    "        for iter in range(num_iter):\n",
    "            dw, db = self.grad(x, y, w, b)\n",
    "            w -= alpha * dw\n",
    "            b -= alpha * db\n",
    "            hist[iter] = self.loss(x, y, w, b)\n",
    "        return w, b, hist\n",
    "    \n",
    "    def bfgs(self,data,param):\n",
    "        'minimize loss function w/ L-BFGS-B'\n",
    "        res = sp.optimize.minimize(self.loss, x0=param, args=data, method='L-BFGS-B',  options={'gtol': 1e-6, 'disp': True})\n",
    "        return res\n",
    "    \n",
    "    def get_stats(self, y, y_hat):\n",
    "        # compare between test fr and model fr\n",
    "        sse = np.sum((y_hat-y)**2);\n",
    "        sst = sum((y-np.mean(y))**2);\n",
    "        varExplain_test = 1-(sse/sst)\n",
    "        r, pval_r = stats.pearsonr(y,y_hat)\n",
    "#         print('sse: ' + str(sse))\n",
    "#         print('sst: ' + str(sst))\n",
    "#         print('varExplain_test: ' + str(varExplain_test))\n",
    "#         print('R: ' + str(r) + ', p=' + str(pval_r))\n",
    "        return sse, sst, varExplain_test, r, pval_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare for battle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize class instance\n",
    "g = glm(ST,P,hd)\n",
    "\n",
    "# prepare the data\n",
    "posgrid_raw,bins = g.pos_map(nbins=10)\n",
    "ebgrid_raw,bins = g.eb_map(nbins=10, rp=[75,75])\n",
    "smooth_fr, raw_spktrn, filt, dt = g.conv_spktrain()\n",
    "posgrid,ebgrid,spiketrain = g.speed_threshold(posgrid_raw,ebgrid_raw,smooth_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into 'nfolds' sections (10)\n",
    "kfoldIdx, kfoldIdx_df = g.kfoldSplit(nfolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateIn = [posgrid,ebgrid]\n",
    "df, expr = g.squish_statemat(stateIn, modelType='PE')\n",
    "# split data into test/train\n",
    "y_train, X_train, y_test, X_test = g.test_train_arr(df,expr,spiketrain)\n",
    "# get initial parameters (beta0 and beta_i's)\n",
    "X = X_train; y = y_train;\n",
    "y = np.squeeze(y)\n",
    "M,n = np.shape(X)\n",
    "w_0 = 1e-3*np.ones((n, )) # initial params\n",
    "b_0 = 1\n",
    "alpha = 0.001\n",
    "data,param = g.getDataParam(X,y,w_0,b_0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize stats matrices \n",
    "nmodels = 3\n",
    "sse = np.zeros((nmodels,1))\n",
    "sst = np.zeros((nmodels,1))\n",
    "varExplain_test = np.zeros((nmodels,1))\n",
    "r = np.zeros((nmodels,1))\n",
    "pval_r = np.zeros((nmodels,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 2))\n",
    "_, bin_edges = np.histogram(y,120)\n",
    "ax[0].plot(bin_edges, scipy.stats.norm.pdf(bin_edges, loc=y.mean(), scale=y.std()))\n",
    "ax[0].set_title(r'Distribution of Rates')\n",
    "ax[0].set_xlabel('rate (hz)')\n",
    "ax[0].set_ylabel('hist')\n",
    "\n",
    "sns.kdeplot(y, color='#fcb103', bw=.017,shade=True)\n",
    "ax[1].set_title(r'Distribution of Rates')\n",
    "ax[1].set_xlabel('rate (hz)')\n",
    "ax[1].set_ylabel('probability');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. PE model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = g.bfgs(data,param)\n",
    "fr_hat_test = g.get_rate(X,res.x[1:],res.x[0])\n",
    "smooth_fr_hat_test = np.convolve(fr_hat_test, filt, mode='same')\n",
    "sse[0], sst[0], varExplain_test[0], r[0], pval_r[0] = g.get_stats(y, smooth_fr_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. P model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p, expr_p = g.squish_statemat(posgrid, modelType='P')\n",
    "# split data into test/train\n",
    "y_train_p, X_train_p, y_test_p, X_test_p = g.test_train_arr(df_p,expr_p,spiketrain)\n",
    "# get initial parameters (beta0 and beta_i's)\n",
    "X = X_train_p; y = y_train_p;\n",
    "y = np.squeeze(y)\n",
    "M,n = np.shape(X)\n",
    "w_0 = 1e-3*np.ones((n, )) # initial params\n",
    "b_0 = 1\n",
    "alpha = 0.001\n",
    "data,param = g.getDataParam(X,y,w_0,b_0);\n",
    "res_p = g.bfgs(data,param)\n",
    "y_hat_p = g.get_rate(X,res_p.x[1:],res_p.x[0])\n",
    "smooth_y_hat_p = np.convolve(y_hat_p, filt, mode='same')\n",
    "sse[1], sst[1], varExplain_test[1], r[1], pval_r[1] = g.get_stats(y, smooth_y_hat_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. E model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e, expr_e = g.squish_statemat(ebgrid, modelType='E')\n",
    "# split data into test/train\n",
    "y_train_e, X_train_e, y_test_e, X_test_e = g.test_train_arr(df_e,expr_e,spiketrain)\n",
    "# get initial parameters (beta0 and beta_i's)\n",
    "X = X_train_e; y = y_train_e;\n",
    "y = np.squeeze(y)\n",
    "M,n = np.shape(X)\n",
    "w_0 = 1e-3*np.ones((n, )) # initial params\n",
    "b_0 = 1\n",
    "alpha = 0.001\n",
    "data,param = g.getDataParam(X,y,w_0,b_0);\n",
    "res_e = g.bfgs(data,param)\n",
    "y_hat_e = g.get_rate(X,res_e.x[1:],res_e.x[0])\n",
    "smooth_y_hat_e = np.convolve(y_hat_e, filt, mode='same')\n",
    "sse[2], sst[2], varExplain_test[2], r[2], pval_r[2] = g.get_stats(y, smooth_y_hat_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(['PE', 'P', 'E'], sse);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_fit = res.x[1:]\n",
    "b_fit = res.x[0]\n",
    "y_hat = g.get_rate(X,w_fit,b_fit)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 8))\n",
    "ax.plot(y[0:10000], label='data');\n",
    "ax.plot(smooth_fr_hat_test[0:10000],label='model');\n",
    "ax.set_title(r'model vs. data')\n",
    "ax.set_xlabel('time (s)')\n",
    "ax.set_ylabel('rate (hz)');\n",
    "ax.legend(loc=\"upper right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-validation stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each section should be 1 minute in length\n",
    "numFolds = 10\n",
    "sections = numFolds*5;\n",
    "bins_1min= int(60/g.dt);\n",
    "nbins = round(len(g.x)/bins_1min);\n",
    "\n",
    "_,numCol = np.shape(X);\n",
    "testFit = np.zeros((numFolds,6)) # columns: var ex, correlation, llh increase, mse, # of spikes, length of test data\n",
    "trainFit = np.zeros((numFolds,6)) #butts\n",
    "paramMat = np.zeros((numFolds,numCol))\n",
    "\n",
    "edges = np.round(np.linspace(1,len(spiketrain)+1,nbins+1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
