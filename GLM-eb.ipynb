{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLM-eb /\n",
    "J. Carpenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load & format data\n",
    "filepath = 'sampleData.mat'\n",
    "mat = scipy.io.loadmat(filepath)\n",
    "\n",
    "ST = mat['ST']; P = mat['P']; hd = mat['hd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class glm:\n",
    "    def __init__(self, ST, P, hd):\n",
    "        self.ST = ST\n",
    "        self.P = P\n",
    "        self.x = P[:,2]\n",
    "        self.y = P[:,3]\n",
    "        self.hd = (hd[:,0]*np.pi)/180; # 0-2pi\n",
    "        \n",
    "    def get_size(self):\n",
    "        boxsz = np.max([np.max(self.x), np.max(self.y)]);\n",
    "        return boxsz\n",
    "    \n",
    "    def pos_map(self, nbins=10):\n",
    "        boxsz = self.get_size()\n",
    "        bins = np.arange(boxsz/nbins/2, boxsz-boxsz/nbins/2, boxsz/nbins)\n",
    "        posgrid = np.zeros((len(self.x), nbins**2))\n",
    "        for idx,val in enumerate(self.x):\n",
    "            xvec = np.abs(self.x[idx]-bins); yvec = np.abs(self.y[idx]-bins);\n",
    "            min_x = np.min(xvec)\n",
    "            min_y = np.min(yvec)\n",
    "            idx_x = np.where(xvec == min_x); idx_x = idx_x[0][0];\n",
    "            idx_y = np.where(yvec == min_y); idx_y = idx_y[0][0];\n",
    "            bin_idx = np.ravel_multi_index((idx_y,idx_x), dims=(nbins,nbins), order='C') # a11=0, a12=1, a13=2;\n",
    "            posgrid[idx, bin_idx] = 1;\n",
    "        return posgrid, bins\n",
    "    \n",
    "    def eb_map(self, nbins=10, rp=[75,75]):\n",
    "        refx = rp[0]; refy = rp[1];\n",
    "        allo = np.arctan2(refy-self.y, refx-self.x) + (np.pi/2); # add 90 deg\n",
    "        allo[allo<0] = allo[allo<0]+2*np.pi;\n",
    "        ego = allo - self.hd; # shift from 0-2pi\n",
    "        egogrid = np.zeros((len(P),nbins));\n",
    "        bins = np.arange(2*np.pi/nbins/2, 2*np.pi-2*np.pi/nbins/2, 2*np.pi/nbins) # 10 bin ctrs\n",
    "        for idx,val in enumerate(P):\n",
    "            evec = np.abs(ego[idx]-bins)\n",
    "            min_e = np.min(evec)\n",
    "            idx_e = np.where(evec == min_e)\n",
    "            egogrid[idx, idx_e] = 1;\n",
    "        return egogrid, bins\n",
    "    \n",
    "    def conv_spktrain(self):\n",
    "        # filter the spiketrain\n",
    "        t = self.P[:,0];\n",
    "        boolean_spk = np.logical_and(t[0] <= self.ST, self.ST <= t[-1])\n",
    "        spikes = self.ST[boolean_spk == True]\n",
    "        edgesT = np.linspace(t[0], t[-1], len(t)+1)\n",
    "        binnedSpikes, timeEdges = np.histogram(spikes, edgesT)\n",
    "        # convolve w/ gaussian membership fn\n",
    "        Xx = np.linspace(-4,4,9); sigma = 2; c = 0;\n",
    "        filt = np.exp((-(Xx-c)**2)/(2*(sigma**2)))\n",
    "        dt = self.P[1,0]-self.P[0,0];\n",
    "        fr = binnedSpikes/dt # rate (hz)\n",
    "        smooth_fr = np.convolve(binnedSpikes, filt, mode='full')\n",
    "        return smooth_fr\n",
    "    \n",
    "    def get_speed(self):\n",
    "        t=self.P[:,0]; x=self.P[:,1]; y=self.P[:,2];\n",
    "        ntime = len(t); v = np.zeros((ntime,1));\n",
    "        for idx in range(1,ntime-1):\n",
    "            v[idx,0] = np.sqrt((x[idx+1]-x[idx-1])**2 + (y[idx+1]-y[idx-1])**2)/(t[idx+1]-t[idx-1])    \n",
    "        v[0,0] = v[1,0]; v[-1,0] = v[-2,0] # pad the array\n",
    "        return v\n",
    "    \n",
    "    def speed_threshold(self,posgrid,ebgrid,spiketrain):\n",
    "        v = self.get_speed()\n",
    "        maxspeed=50; minspeed=4\n",
    "        inbounds = np.logical_and((v<=maxspeed), (v>=minspeed))\n",
    "        inbounds = np.where(inbounds==True); inbounds = inbounds[0]\n",
    "        posgrid = posgrid[inbounds,:]\n",
    "        ebgrid = ebgrid[inbounds,:]\n",
    "        spiketrain = spiketrain[inbounds]\n",
    "        return posgrid, ebgrid, spiketrain\n",
    "    \n",
    "    def squish_statemat(self,posgrid,ebgrid):\n",
    "        '''squish state matrix for 2-variable model (P+EB)'''\n",
    "        ntime,nbins_eb = np.shape(ebgrid)\n",
    "        _,nbins_p = np.shape(posgrid)\n",
    "        A = np.zeros((ntime, nbins_p+nbins_eb)) #P+EB\n",
    "        A[:,0:nbins_p] = posgrid; A[:,nbins_p:] = ebgrid\n",
    "        df=pd.DataFrame(A)\n",
    "        mask = np.random.rand(len(df)) < 0.8\n",
    "        df_train = df[mask]; df_test = df[~mask]\n",
    "        # name columns & get expression\n",
    "        colnames = [];\n",
    "        expr = 'y ~ '\n",
    "        for i in range(nbins_p):\n",
    "            val = str(i);\n",
    "            expr = expr + 'P' + val + ' + '\n",
    "            colnames.append('P' + val)\n",
    "        for i in range(nbins_eb-1):\n",
    "            val = str(i);\n",
    "            expr = expr + 'E' + val + ' + '\n",
    "            colnames.append('E' + val)\n",
    "        expr = expr + 'E9'\n",
    "        colnames.append('E9')\n",
    "        df.columns = colnames\n",
    "        return df,expr\n",
    "    \n",
    "    def test_train(self,df,expr,spiketrain):\n",
    "        df.insert(loc=0, column='y', value=spiketrain, allow_duplicates=False)\n",
    "        mask = np.random.rand(len(df)) < 0.8\n",
    "        df_train = df[mask]\n",
    "        df_test = df[~mask]\n",
    "        # split into test and train\n",
    "        y_train, X_train = dmatrices(expr, df_train, return_type='dataframe')\n",
    "        y_test, X_test = dmatrices(expr, df_test, return_type='dataframe')\n",
    "        # info for user\n",
    "        print('Training data set length='+str(len(df_train)))\n",
    "        print('Testing data set length='+str(len(df_test)))\n",
    "        return y_train, X_train, y_test, X_test\n",
    "    \n",
    "    def init_params(self,whichVars={'P', 'E'}):\n",
    "        if whichVars == {'P', 'E'}: init_param = 1e-3*np.random.randn(110+1, 1);\n",
    "        if whichVars == {'P'}: init_param = 1e-3*np.random.randn(100+1, 1);\n",
    "        if whichVars == {'E'}: init_param = 1e-3*np.random.randn(10+1, 1);\n",
    "        return init_param\n",
    "    \n",
    "    def get_rate(self,X,param):\n",
    "        u = np.dot(X,param);\n",
    "        rate = np.exp(u);\n",
    "        return rate,u\n",
    "    \n",
    "    def get_LL(self,rate,Y):\n",
    "        '''log-likelihood function'''\n",
    "        loglik = np.sum(Y*np.log(rate)-rate)\n",
    "        return loglik\n",
    "    \n",
    "    ##################################################################################################################\n",
    "    \n",
    "    # functions from the tutorial; will probably change these to simplify (?)\n",
    "    \n",
    "    def qu(self,z):\n",
    "        '''the nonlinearity (w/ softplus)'''\n",
    "        qu = np.log1p(np.exp(z))\n",
    "        \n",
    "    def lmb(self,beta0,beta,X):\n",
    "        '''conditional intensity function'''\n",
    "        z = beta0 + np.dot(X,beta)\n",
    "        l = self.qu(z)\n",
    "        return l\n",
    "    \n",
    "    def penalty(self,alpha,beta):\n",
    "        '''penalty term'''\n",
    "        P = 0.5 * (1 - alpha) * np.linalg.norm(beta, 2) ** 2 + \\\n",
    "        alpha * np.linalg.norm(beta, 1)\n",
    "        return P\n",
    "    \n",
    "    def loss(self,beta0, beta, reg_lambda, X, y):\n",
    "        '''define objective function for elastic net'''\n",
    "        L = logL(beta0, beta, X, y)\n",
    "        P = self.penalty(beta)\n",
    "        J = -L + reg_lambda * P\n",
    "        return J\n",
    "    \n",
    "    def grad_L2loss(self,beta0, beta, reg_lambda, X, y):\n",
    "        z = beta0 + np.dot(X, beta)\n",
    "        s = expit(z)\n",
    "        q = self.qu(z)\n",
    "        grad_beta0 = np.sum(s) - np.sum(y * s / q)\n",
    "        grad_beta = np.transpose(np.dot(np.transpose(s), X) -\n",
    "                                 np.dot(np.transpose(y * s / q), X)) + \\\n",
    "        reg_lambda * (1 - alpha) * beta\n",
    "        return grad_beta0, grad_beta\n",
    "    \n",
    "    def hessian_loss(beta0, beta, alpha, reg_lambda, X, y):\n",
    "        z = beta0 + np.dot(X, beta)\n",
    "        q = qu(z)\n",
    "        s = expit(z)\n",
    "        grad_s = s * (1-s)\n",
    "        grad_s_by_q = grad_s/q - s/(q * q)\n",
    "        hess_beta0 = np.sum(grad_s) - np.sum(y * grad_s_by_q)\n",
    "        hess_beta = np.transpose(np.dot(np.transpose(grad_s), X * X)\n",
    "                                - np.dot(np.transpose(y * grad_s_by_q), X * X))\\\n",
    "                                + reg_lambda * (1-alpha)\n",
    "        return hess_beta0, hess_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### apply methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data set length=38965\n",
      "Testing data set length=9825\n"
     ]
    }
   ],
   "source": [
    "# initialize class instance\n",
    "g = glm(ST,P,hd)\n",
    "\n",
    "# prepare the data\n",
    "posgrid_raw,bins = g.pos_map(nbins=10)\n",
    "ebgrid_raw,bins = g.eb_map(nbins=10, rp=[75,75])\n",
    "smooth_fr = g.conv_spktrain()\n",
    "posgrid,ebgrid,spiketrain = g.speed_threshold(posgrid_raw,ebgrid_raw,smooth_fr)\n",
    "df,expr = g.squish_statemat(posgrid,ebgrid)\n",
    "\n",
    "# split data into test/train\n",
    "y_train, X_train, y_test, X_test = g.test_train(df,expr,spiketrain)\n",
    "\n",
    "# get initial parameters (beta0 and beta_i's)\n",
    "init_param = g.init_params(whichVars={'P', 'E'})\n",
    "param = init_param\n",
    "X = X_train; Y = y_train;\n",
    "rate,u = g.get_rate(X,param)\n",
    "\n",
    "# minimize -loglikelihood fn\n",
    "loglik = g.get_LL(rate,Y)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
