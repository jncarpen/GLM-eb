{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM-eb \n",
    "@author: Jordan, Ben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble\n",
    "import scipy.io\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "import scipy.sparse as sps\n",
    "import scipy.stats as stats\n",
    "from pyglmnet import GLM, simulate_glm\n",
    "import scipy as sp\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization parameters\n",
    "plt.rcParams['figure.figsize'] = (4,2)\n",
    "plt.rc('axes', labelsize=10); plt.rc('axes', titlesize=10)\n",
    "plt.style.use('ggplot'); plt.rc('font', size=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load & format data\n",
    "filepath = 'sampleData.mat'\n",
    "mat = scipy.io.loadmat(filepath)\n",
    "ST = mat['ST']; P = mat['P']; hd = mat['hd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class glm:\n",
    "    def __init__(self, ST, P, hd):\n",
    "        self.ST = ST\n",
    "        self.P = P\n",
    "        self.x = P[:,1]\n",
    "        self.y = P[:,2]\n",
    "        self.t = P[:,0]\n",
    "        self.hd = (hd[:,0]*np.pi)/180; # 0-2pi\n",
    "        self.dt = np.round(statistics.mode(np.diff(P[:,0])),2);\n",
    "        \n",
    "    def get_size(self):\n",
    "        '''get size of recording box'''\n",
    "        boxsz = np.max([np.max(self.x), np.max(self.y)]);\n",
    "        return boxsz\n",
    "    \n",
    "    def pos_map(self, nbins=10):\n",
    "        '''design matrix for position variables'''\n",
    "        boxsz = self.get_size()\n",
    "        bins = np.arange(boxsz/nbins/2, boxsz-boxsz/nbins/2, boxsz/nbins)\n",
    "        posgrid = np.zeros((len(self.x), nbins**2))\n",
    "        for idx,val in enumerate(self.x):\n",
    "            xvec = np.abs(self.x[idx]-bins); yvec = np.abs(self.y[idx]-bins);\n",
    "            min_x = np.min(xvec)\n",
    "            min_y = np.min(yvec)\n",
    "            idx_x = np.where(xvec == min_x); idx_x = idx_x[0][0];\n",
    "            idx_y = np.where(yvec == min_y); idx_y = idx_y[0][0];\n",
    "            bin_idx = np.ravel_multi_index((idx_y,idx_x), dims=(nbins,nbins), order='C') # a11=0, a12=1, a13=2;\n",
    "            posgrid[idx, bin_idx] = 1;\n",
    "        return posgrid, bins\n",
    "    \n",
    "    def eb_map(self, nbins=10, rp=[75,75]):\n",
    "        '''design matrix for egocentric variables'''\n",
    "        refx = rp[0]; refy = rp[1];\n",
    "        allo = np.arctan2(refy-self.y, refx-self.x) + (np.pi/2); # add 90 deg\n",
    "        allo[allo<0] = allo[allo<0]+2*np.pi;\n",
    "        ego = allo - self.hd; # shift from 0-2pi\n",
    "        egogrid = np.zeros((len(P),nbins));\n",
    "        bins = np.arange(2*np.pi/nbins/2, 2*np.pi-2*np.pi/nbins/2, 2*np.pi/nbins) # 10 bin ctrs\n",
    "        for idx,val in enumerate(P):\n",
    "            evec = np.abs(ego[idx]-bins)\n",
    "            min_e = np.min(evec)\n",
    "            idx_e = np.where(evec == min_e)\n",
    "            egogrid[idx, idx_e] = 1;\n",
    "        return egogrid, bins\n",
    "    \n",
    "    def conv_spktrain(self, Xx=np.linspace(-4,4,9),\n",
    "                      sigma=2,c=0,defaultST=True,spikeIn=[1,2,3],dt=0.02):\n",
    "        '''get smoothed spiketrain from spiketimes\n",
    "            **kwargs:\n",
    "            spikeTrain- 'False' if user wants self.ST (spiketimes)\n",
    "                        'True' if user wants to use a pre-allocated spiketrain\n",
    "            spikeIn-    use this optional kwarg iff spikeTrain==True\n",
    "        '''\n",
    "        if defaultST==True:\n",
    "            t = self.P[:,0]; dt = self.P[1,0]-self.P[0,0]; # time per frame\n",
    "            boolean_spk = np.logical_and(t[0] <= self.ST, self.ST <= t[-1])\n",
    "            spikes = self.ST[boolean_spk == True]\n",
    "            edgesT = np.linspace(t[0], t[-1], len(t)+1)\n",
    "            binnedSpikes, timeEdges = np.histogram(spikes, edgesT)\n",
    "            \n",
    "        elif defaultST==False:\n",
    "            binnedSpikes = spikeIn\n",
    "            \n",
    "        # convolve w/ gaussian membership function\n",
    "        filt = np.exp((-(Xx-c)**2)/(2*(sigma**2)))\n",
    "        fr = binnedSpikes/dt # rate (hz)\n",
    "        smooth_fr = np.convolve(binnedSpikes, filt, mode='same')\n",
    "        \n",
    "        return smooth_fr, binnedSpikes, filt, dt\n",
    "\n",
    "    \n",
    "    def get_speed(self):\n",
    "        '''get speed of the animal (cm*s^-2)'''\n",
    "        t=self.P[:,0]; x=self.P[:,1]; y=self.P[:,2];\n",
    "        ntime = len(t); v = np.zeros((ntime,1));\n",
    "        \n",
    "        for idx in range(1,ntime-1):\n",
    "            v[idx,0] = np.sqrt((x[idx+1]-x[idx-1])**2 + (y[idx+1]-y[idx-1])**2)/(t[idx+1]-t[idx-1])    \n",
    "        v[0,0] = v[1,0]; v[-1,0] = v[-2,0] # pad the array\n",
    "        \n",
    "        return v\n",
    "    \n",
    "    def speed_threshold(self,posgrid,ebgrid,spiketrain):\n",
    "        v = self.get_speed()\n",
    "        maxspeed=50; minspeed=4\n",
    "        inbounds = np.logical_and((v<=maxspeed), (v>=minspeed))\n",
    "        inbounds = np.where(inbounds==True); inbounds = inbounds[0]\n",
    "        posgrid = posgrid[inbounds,:]\n",
    "        ebgrid = ebgrid[inbounds,:]\n",
    "        spiketrain = spiketrain[inbounds]\n",
    "        return posgrid, ebgrid, spiketrain\n",
    "    \n",
    "    def squish_statemat(self, spiketrain, stateIn, modelType='PE'):\n",
    "        '''squish state matrix for 2-variable model (P+EB)\n",
    "            inputs- spiketrain is the speed-thresholded spiketrain'''\n",
    "        if modelType == 'PE':\n",
    "            posgrid = stateIn[0]; ebgrid = stateIn[1]\n",
    "            ntime,nbins_eb = np.shape(ebgrid)\n",
    "            _,nbins_p = np.shape(posgrid)\n",
    "            A = np.zeros((ntime, nbins_p+nbins_eb)) #P+EB\n",
    "            A[:,0:nbins_p] = posgrid; A[:,nbins_p:] = ebgrid\n",
    "            df=pd.DataFrame(A)\n",
    "            # name columns & get expression\n",
    "            colnames = [];\n",
    "            expr = 'y ~ '\n",
    "            for i in range(nbins_p):\n",
    "                val = str(i);\n",
    "                expr = expr + 'P' + val + ' + '\n",
    "                colnames.append('P' + val)\n",
    "            for i in range(nbins_eb-1):\n",
    "                val = str(i);\n",
    "                expr = expr + 'E' + val + ' + '\n",
    "                colnames.append('E' + val)\n",
    "            expr = expr + 'E9'\n",
    "            colnames.append('E9')\n",
    "            df.columns = colnames\n",
    "        elif modelType == 'P':\n",
    "            ntime,nbins = np.shape(stateIn)\n",
    "            df = pd.DataFrame(stateIn)\n",
    "            colnames = [];\n",
    "            expr = 'y ~ '\n",
    "            for i in range(nbins-1):\n",
    "                val = str(i);\n",
    "                expr = expr + 'P' + val + ' + '\n",
    "                colnames.append('P' + val)\n",
    "            expr = expr + 'P99'\n",
    "            colnames.append('P99')\n",
    "            df.columns = colnames\n",
    "        elif modelType == 'E':\n",
    "            ntime,nbins = np.shape(stateIn)\n",
    "            df = pd.DataFrame(stateIn)\n",
    "            colnames = [];\n",
    "            expr = 'y ~ '\n",
    "            for i in range(nbins-1):\n",
    "                val = str(i);\n",
    "                expr = expr + 'E' + val + ' + '\n",
    "                colnames.append('E' + val)\n",
    "            expr = expr + 'E9'\n",
    "            colnames.append('E9')\n",
    "            df.columns = colnames\n",
    "        else:\n",
    "            print('Error: model type must be \"P\", \"E\", or \"PE\"')\n",
    "        # if you want to do a 20-80 test-train split\n",
    "        # note: make this an option\n",
    "        mask = np.random.rand(len(df)) < 0.8\n",
    "        df_train = df[mask]; df_test = df[~mask]\n",
    "        # insert [raw] spiketrain into dataframe\n",
    "        df.insert(0, 'y', spiketrain)\n",
    "        return df,expr\n",
    "    \n",
    "    def kfoldSplit(self,nfolds=10):\n",
    "        '''train-test split for k-fold xval\n",
    "            each section is ~1 min'''\n",
    "        _, spiketrain, _, dt = self.conv_spktrain()\n",
    "        # calculate number of chunks given session length\n",
    "        nmins = (len(spiketrain)*dt)/60\n",
    "        nchunks = int(round(nmins/nfolds))\n",
    "        nsections = int(nchunks*nfolds)\n",
    "        # grab indices for k-fold splitting\n",
    "        kfoldIdx = {}\n",
    "        howLong = np.zeros(nfolds)\n",
    "        edges = np.round(np.linspace(1,len(spiketrain)+1,nsections+1))\n",
    "        for k in range(nfolds):\n",
    "            test_ind = np.floor(np.linspace(int(edges[k]),\n",
    "                    (int(edges[k+1])-1),\n",
    "                    (int(edges[k+1])-1)-int(edges[k])))\n",
    "            for s in range(1,nchunks):\n",
    "                ind = np.floor(np.linspace(int(edges[k+s*nfolds]),\n",
    "                                  (int(edges[k+s*nfolds+1])-1),\n",
    "                                  (int(edges[k+s*nfolds+1]))-int(edges[k+s*nfolds])))\n",
    "                test_ind = np.append(test_ind,ind)\n",
    "            kfoldIdx[k] = test_ind\n",
    "            howLong[k] = len(test_ind); del test_ind;\n",
    "        minArrLen = int(np.min(howLong));\n",
    "        for k in range(nfolds):\n",
    "            kfoldIdx[k] = kfoldIdx[k][0:minArrLen] # adjust arr. len (w/in .02 s)\n",
    "        kfoldIdx_df = pd.DataFrame.from_dict(kfoldIdx)\n",
    "        kfoldIdx_df = kfoldIdx_df.astype(int) # for idxing purposes\n",
    "        return kfoldIdx, kfoldIdx_df\n",
    "    \n",
    "    def kfoldOptim(self,kfoldIdx_df,statemat):\n",
    "        '''kfoldIdx_df can be retrieved from self.kfoldSplit()'''\n",
    "        \n",
    "        # intialize output structures\n",
    "        _,nfolds=np.shape(kfoldIdx_df); k_vec = np.arange(nfolds)\n",
    "        kres = {}; train_y = {}; train_x = {}; test_y = {}\n",
    "        test_x = {}; train_y_raw = {}; test_y_raw = {}\n",
    "        \n",
    "        for foldnum in range(nfolds):\n",
    "            k_vec_train = np.delete(k_vec, np.where(k_vec == foldnum))\n",
    "            idx_test = kfoldIdx_df.loc[:,foldnum].to_numpy()\n",
    "            idx_train = []\n",
    "            \n",
    "            # squeeze other nfolds-1 folds into one vector\n",
    "            for i,v in enumerate(k_vec_train):\n",
    "                nextRow = kfoldIdx_df.loc[:,v].to_numpy()\n",
    "                idx_train = np.append(idx_train,nextRow) \n",
    "            idx_train = idx_train.astype(int)\n",
    "            \n",
    "            # train-test statemats\n",
    "            df_test = statemat.loc[idx_test,:].dropna()\n",
    "            df_train = statemat.loc[idx_train,:].dropna()\n",
    "            y_test_raw = df_test['y'].to_numpy(dtype='int64');\n",
    "            y_train_raw = df_train['y'].to_numpy(dtype='int64')\n",
    "            \n",
    "            # smooth firing rates\n",
    "            y_test, _, _, _ = g.conv_spktrain(defaultST=False,spikeIn=y_test_raw)\n",
    "            y_train, _, _, _ = g.conv_spktrain(defaultST=False,spikeIn=y_train_raw) \n",
    "            \n",
    "            # put smoothed firing rates back into dataframe\n",
    "            df_test[df_test.columns[0]] = y_test; \n",
    "            df_train[df_train.columns[0]] = y_train\n",
    "            \n",
    "            # test/train arrays\n",
    "            X_test = df_test[df_test.columns[1:]].to_numpy(); y_test = df_test[df_test.columns[0]].to_numpy()\n",
    "            X_train = df_train[df_train.columns[1:]].to_numpy(); y_train = df_train[df_train.columns[0]].to_numpy()\n",
    "            \n",
    "            # set some initial parameters\n",
    "            M,n = np.shape(X_train)\n",
    "            w_0 = 1e-3*np.ones((n, ))\n",
    "            b_0 = 1\n",
    "            # alpha = 0.001 (can't remember when we use this)\n",
    "            \n",
    "            # get parameters & jacobian (1st order derivatives of loss fn)\n",
    "            data,param = self.getDataParam(X_train,y_train,w_0,b_0);\n",
    "            jac = self.grad(param,X_train,y_train)\n",
    "            \n",
    "            # optimize loss function\n",
    "            res = self.bfgs(data,param)\n",
    "            \n",
    "            # package outputs for each fold\n",
    "            kres[foldnum] = res\n",
    "            train_y[foldnum] = y_train; test_y[foldnum] = y_test\n",
    "            train_x[foldnum] = X_train; test_x[foldnum] = X_test\n",
    "            train_y_raw[foldnum] = y_train_raw; \n",
    "            test_y_raw[foldnum] = y_test_raw; \n",
    "            \n",
    "            del y_test_raw, y_train_raw # for debugging\n",
    "        return kres,train_y, test_y, train_x, test_x, train_y_raw, test_y_raw\n",
    "    \n",
    "    def init_params(self,whichVars={'P', 'E'}):\n",
    "        if whichVars == {'P', 'E'}: init_param = 1e-3*np.random.randn(110, 1);\n",
    "        if whichVars == {'P'}: init_param = 1e-3*np.random.randn(100, 1);\n",
    "        if whichVars == {'E'}: init_param = 1e-3*np.random.randn(10, 1);\n",
    "        return init_param\n",
    "    \n",
    "    def getDataParam(self,x,y,w,b):\n",
    "        '''put param & data in a dictionary'''\n",
    "        param = np.append(b,w)\n",
    "        data =  (x, y)\n",
    "        return data,param\n",
    "    \n",
    "    def get_rate(self,x,w,b):\n",
    "        '''conditional intensity function'''\n",
    "        # note: not normalized by dt (not in Hz)\n",
    "        y_hat = np.exp(x @ w + b)\n",
    "        return y_hat\n",
    "    \n",
    "    def loss(self,param,x,y):\n",
    "        '''objective function'''\n",
    "        \n",
    "        y_hat = np.exp(x @ param[1:] + param[0])\n",
    "        #negative log likelihood for possion where yhat is lambda\n",
    "        error = (y_hat - np.log(y_hat) * y).mean()\n",
    "        \n",
    "        return error\n",
    "    \n",
    "    def grad(self,param,x,y):\n",
    "        '''compute the gradient of the loss fn'''\n",
    "        M, n = x.shape\n",
    "        y_hat = np.exp(x @ param[1:] + param[0])\n",
    "        dw = (x.T @ (y_hat - y)) / M\n",
    "        db = (y_hat - y).mean() \n",
    "        # we dont really need [db] if the bias term is included as a column of 1's\n",
    "        # which right now it is currently NOT\n",
    "        jac = dw; jac=np.append(jac,db);\n",
    "        return jac\n",
    "\n",
    "    def gradient_descent(self,x, y, w_0, b_0, alpha, num_iter):\n",
    "        '''minimize loss function w/ gradient descent'''\n",
    "        w, b = w_0.copy(), b_0\n",
    "        hist = np.zeros(num_iter)\n",
    "        M, n = x.shape\n",
    "        for iter in range(num_iter):\n",
    "            dw, db = self.grad(x, y, w, b)\n",
    "            w -= alpha * dw\n",
    "            b -= alpha * db\n",
    "            hist[iter] = self.loss(x, y, w, b)\n",
    "        return w, b, hist\n",
    "    \n",
    "    def bfgs(self,data,param):\n",
    "        'minimize loss function w/ L-BFGS-B'\n",
    "        res = sp.optimize.minimize(self.loss, x0=param, args=data, method='L-BFGS-B', jac=self.grad)\n",
    "        # options={'gtol': 1e-6, 'disp': True})\n",
    "        return res\n",
    "    \n",
    "    def get_stats(self, y, y_hat):\n",
    "        # compare between test fr and model fr\n",
    "        sse = np.sum((y_hat-y)**2);\n",
    "        sst = sum((y-np.mean(y))**2);\n",
    "        varExplain_test = 1-(sse/sst)\n",
    "        r, pval_r = stats.pearsonr(y,y_hat)\n",
    "        return sse, sst, varExplain_test, r, pval_r\n",
    "    \n",
    "    def get_testFit(self,kres,train_y,test_y,train_x,test_x,train_y_raw,test_y_raw):\n",
    "        '''get statistics for model fit'''\n",
    "        # initialize output structures\n",
    "        yhatDict={}\n",
    "        nfolds=len(kres);\n",
    "        sse=np.zeros(nfolds); sst=np.zeros(nfolds)\n",
    "        varExplain_test=np.zeros(nfolds); pearson_r=np.zeros(nfolds)\n",
    "        pearson_pval=np.zeros(nfolds); funval=np.zeros(nfolds)\n",
    "        llh=np.zeros(nfolds)\n",
    "        \n",
    "        for fold in range(nfolds):\n",
    "            # predict model output on *test* data\n",
    "            yhat_raw = g.get_rate(test_x[fold],kres[fold].x[1:],kres[fold].x[0]) # not normalized\n",
    "            yhat, _, _, _ = g.conv_spktrain(defaultST=False,spikeIn=yhat_raw) # normalized by dt (hz)\n",
    "            \n",
    "            # compare between test and model rates\n",
    "            sse[fold] = np.sum((yhat-test_y[fold])**2); #sse\n",
    "            sst[fold]= sum((test_y[fold]-np.mean(test_y[fold]))**2); #sst\n",
    "            varExplain_test[fold] = 1-(sse[fold]/sst[fold]) #varExplained_test\n",
    "            pearson_r[fold],pearson_pval[fold] = stats.pearsonr(test_y[fold],yhat) #pearsonsR,p-val\n",
    "            funval[fold] = kres[fold].fun\n",
    "            yhatDict[fold] = yhat\n",
    "            \n",
    "            #compute llh increase from \"mean firing rate model\"\n",
    "            # NO SMOOTHING is used here\n",
    "            bestp = kres[fold]['x'] # best parameters\n",
    "            n = test_y_raw[fold]\n",
    "            arrFactorial = np.vectorize(math.factorial) # array-wise factorial fn\n",
    "            meanFR_test = np.nanmean(n)\n",
    "\n",
    "            # format the state matrix\n",
    "            b = np.ones((len(test_x[fold]),len(bestp)))\n",
    "            b[:,1:] = test_x[fold]\n",
    "            # get rate of 'mean fr model'\n",
    "            r = np.exp(b@bestp) # predicted rate (not normalized by dt)\n",
    "            \n",
    "            # compute log-likelihood value for the test data\n",
    "            log_llh_test_model = np.nansum(r-n*np.log(r)+np.log(arrFactorial(n)))/np.sum(n);\n",
    "            log_llh_test_mean = np.nansum(meanFR_test-n*np.log(meanFR_test)+np.log(arrFactorial(n)))/np.sum(n);\n",
    "            log_llh_test = (-log_llh_test_model + log_llh_test_mean);\n",
    "            log_llh_test = np.log(2)*log_llh_test;\n",
    "            llh[fold] = log_llh_test\n",
    "            \n",
    "            # akaike info criterion\n",
    "            # log is undefined b/c llh is negative\n",
    "            # AIC = 2*(len(bestp))-2*np.log(log_llh_test);\n",
    "            \n",
    "        # dictionary of statistics describing fit of test data to model\n",
    "        testfit = {\n",
    "            'llh_test': llh,\n",
    "            'sse': sse,\n",
    "            'sst': sst,\n",
    "            'varEx': varExplain_test,\n",
    "            'pearson_r': pearson_r,\n",
    "            'pearson_pval': pearson_pval,\n",
    "            'funval': funval,\n",
    "            'yhat': yhatDict\n",
    "        }\n",
    "        return testfit\n",
    "    \n",
    "    def findBestModel(self,modelDict):\n",
    "        '''modelDict is defined in the script below (incorporate)'''\n",
    "        numModels = len(allModels)\n",
    "        llh = np.zeros(numModels)\n",
    "        for model in range(numModels):\n",
    "            T = allModels[model]['testfit']\n",
    "            llh[model] = np.nanmean(T['llh_test'])\n",
    "        bestModel=np.where(llh==np.max(llh))[0][0]\n",
    "        print('best model: ' + labelDict[bestModel])\n",
    "        \n",
    "        return llh, bestModel\n",
    "    \n",
    "    def plot_llh(self,allModels,labelDict):\n",
    "        # plot log-likelihood values for each model\n",
    "        numModels = len(allModels)\n",
    "        nfolds = len(allModels[0]['kres'].keys())\n",
    "        \n",
    "        llh = {}; whichModel = {}\n",
    "        for model in range(numModels):\n",
    "            a = allModels[model]['testfit']['llh_test']\n",
    "            a = a[None].T\n",
    "            llh[model] = a\n",
    "#             labels = np.ones((nfolds,1));\n",
    "#             labels[:] = labelDict[model]\n",
    "            whichModel[model] = np.ones((nfolds,1))*model\n",
    "\n",
    "        llh = np.vstack((llh[0], llh[1], llh[2]))\n",
    "        whichModel = np.vstack((whichModel[0], whichModel[1], whichModel[2]))\n",
    "        df = pd.DataFrame(np.hstack((llh,whichModel)), columns = ['llh','whichModel'])\n",
    "        ax = sns.swarmplot(x=\"whichModel\", y=\"llh\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### execute glm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17145\\.conda\\envs\\tdt4195\\lib\\site-packages\\ipykernel_launcher.py:264: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\17145\\.conda\\envs\\tdt4195\\lib\\site-packages\\ipykernel_launcher.py:264: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\Users\\17145\\.conda\\envs\\tdt4195\\lib\\site-packages\\ipykernel_launcher.py:262: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Users\\17145\\.conda\\envs\\tdt4195\\lib\\site-packages\\ipykernel_launcher.py:271: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Users\\17145\\.conda\\envs\\tdt4195\\lib\\site-packages\\ipykernel_launcher.py:272: RuntimeWarning: invalid value encountered in matmul\n",
      "C:\\Users\\17145\\.conda\\envs\\tdt4195\\lib\\site-packages\\ipykernel_launcher.py:264: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model: E\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize class instance\n",
    "g = glm(ST,P,hd)\n",
    "\n",
    "# prepare the data\n",
    "posgrid_raw,bins = g.pos_map(nbins=10)\n",
    "ebgrid_raw,bins = g.eb_map(nbins=10, rp=[75,75])\n",
    "smooth_fr, raw_spktrn, filt, dt = g.conv_spktrain() # get spiketrain\n",
    "posgrid,ebgrid,spiketrain = g.speed_threshold(posgrid_raw,ebgrid_raw,raw_spktrn)\n",
    "\n",
    "# dictionaries with info about each model\n",
    "stateDict = {\n",
    "    0: [posgrid,ebgrid],\n",
    "    1: posgrid,\n",
    "    2: ebgrid\n",
    "}\n",
    "\n",
    "labelDict = {\n",
    "    0: 'PE',\n",
    "    1: 'P',\n",
    "    2: 'E'\n",
    "}\n",
    "\n",
    "allModels = {}\n",
    "numModels = 3\n",
    "\n",
    "# get test/train indices (same for each model)\n",
    "kfoldIdx, kfoldIdx_df = g.kfoldSplit(nfolds=10)\n",
    "\n",
    "for model in range(numModels):\n",
    "    modelDict = {}\n",
    "    # get state matrix\n",
    "    stateIn = stateDict[model]\n",
    "    statemat, expr = g.squish_statemat(spiketrain, stateIn, modelType=labelDict[model])\n",
    "\n",
    "    # optimize model parameters\n",
    "    kres,train_y, test_y, train_x, test_x, train_y_raw, test_y_raw= g.kfoldOptim(kfoldIdx_df,statemat)\n",
    "\n",
    "    # check the model fit\n",
    "    testfit = g.get_testFit(kres,train_y,test_y,train_x,test_x,train_y_raw,test_y_raw)\n",
    "\n",
    "    modelDict['kfoldIdx'] = kfoldIdx_df\n",
    "    modelDict['kres'] = kres\n",
    "    modelDict['train_y'] = train_y\n",
    "    modelDict['train_x'] = train_x\n",
    "    modelDict['test_y'] = test_y\n",
    "    modelDict['test_x'] = test_x\n",
    "    modelDict['train_y_raw'] = train_y_raw\n",
    "    modelDict['test_y_raw'] = test_y_raw\n",
    "    modelDict['testfit'] = testfit\n",
    "    \n",
    "    # save in allModels dictionary\n",
    "    allModels[model] = modelDict\n",
    "\n",
    "llh, bestModel = g.findBestModel(modelDict)\n",
    "\n",
    "g.plot_llh(allModels,labelDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1);\n",
    "ax.plot(test_y[fold]); ax.plot(yhat);\n",
    "ax.set_xlabel('time step'); ax.set_ylabel('y (smoothed rate)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 2))\n",
    "_, bin_edges = np.histogram(y,120)\n",
    "ax[0].plot(bin_edges, scipy.stats.norm.pdf(bin_edges, loc=y.mean(), scale=y.std()))\n",
    "ax[0].set_title(r'Distribution of Rates')\n",
    "ax[0].set_xlabel('rate (hz)')\n",
    "ax[0].set_ylabel('hist')\n",
    "\n",
    "sns.kdeplot(y, color='#fcb103', bw=.017,shade=True)\n",
    "ax[1].set_title(r'Distribution of Rates')\n",
    "ax[1].set_xlabel('rate (hz)')\n",
    "ax[1].set_ylabel('probability');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(['PE', 'P', 'E'], sse);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_fit = res.x[1:]\n",
    "b_fit = res.x[0]\n",
    "y_hat = g.get_rate(X,w_fit,b_fit)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 8))\n",
    "ax.plot(y[0:10000], label='data');\n",
    "ax.plot(smooth_fr_hat_test[0:10000],label='model');\n",
    "ax.set_title(r'model vs. data')\n",
    "ax.set_xlabel('time (s)')\n",
    "ax.set_ylabel('rate (hz)');\n",
    "ax.legend(loc=\"upper right\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
