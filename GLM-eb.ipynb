{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLM-eb \n",
    "@author: Jordan, Ben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preamble\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import scipy as sp\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glmmod\n",
    "# should be stored in the same folder *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLM MODULE\n",
    "# @author: jo carpenter\n",
    "\n",
    "# preamble\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import scipy as sp\n",
    "import statistics\n",
    "from scipy.sparse import spdiags, csr_matrix\n",
    "\n",
    "class glm:\n",
    "    def __init__(self, ST, P, hd):\n",
    "        self.ST = ST\n",
    "        self.P = P\n",
    "        self.x = P[:,1]\n",
    "        self.y = P[:,2]\n",
    "        self.t = P[:,0]\n",
    "        self.hd = (hd[:,0]*np.pi)/180; # 0-2pi\n",
    "        self.dt = np.round(statistics.mode(np.diff(P[:,0])),2)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_size(self):\n",
    "        '''get size of recording box'''\n",
    "        \n",
    "        boxsz = np.max([np.max(self.x), np.max(self.y)])\n",
    "        \n",
    "        return boxsz\n",
    "    \n",
    "    \n",
    "    \n",
    "    def pos_map(self, nbins=10):\n",
    "        '''design matrix for position variables'''\n",
    "        \n",
    "        boxsz = self.get_size()\n",
    "        bins = np.arange(boxsz/nbins/2, boxsz-boxsz/nbins/2, boxsz/nbins)\n",
    "        posgrid = np.zeros((len(self.x), nbins**2))\n",
    "        \n",
    "        for idx,val in enumerate(self.x):\n",
    "            \n",
    "            xvec = np.abs(self.x[idx]-bins); yvec = np.abs(self.y[idx]-bins);\n",
    "            min_x = np.min(xvec)\n",
    "            min_y = np.min(yvec)\n",
    "            idx_x = np.where(xvec == min_x); idx_x = idx_x[0][0];\n",
    "            idx_y = np.where(yvec == min_y); idx_y = idx_y[0][0];\n",
    "            bin_idx = np.ravel_multi_index((idx_y,idx_x), dims=(nbins,nbins), order='C') # a11=0, a12=1, a13=2;\n",
    "            posgrid[idx, bin_idx] = 1;\n",
    "            \n",
    "        return posgrid, bins\n",
    "    \n",
    "    \n",
    "    \n",
    "    def eb_map(self, nbins=10, rp=[75,75]):\n",
    "        '''design matrix for egocentric variables'''\n",
    "        \n",
    "        refx = rp[0]; refy = rp[1];\n",
    "        allo = np.arctan2(refy-self.y, refx-self.x) + (np.pi/2); # add 90 deg\n",
    "        allo[allo<0] = allo[allo<0]+2*np.pi;\n",
    "        ego = allo - self.hd; # shift from 0-2pi\n",
    "        egogrid = np.zeros((len(self.P),nbins));\n",
    "        bins = np.arange(2*np.pi/nbins/2, 2*np.pi-2*np.pi/nbins/2, 2*np.pi/nbins) # 10 bin ctrs\n",
    "        \n",
    "        for idx,val in enumerate(self.P):\n",
    "            \n",
    "            evec = np.abs(ego[idx]-bins)\n",
    "            min_e = np.min(evec)\n",
    "            idx_e = np.where(evec == min_e)\n",
    "            egogrid[idx, idx_e] = 1;\n",
    "            \n",
    "        return egogrid, bins\n",
    "    \n",
    "    \n",
    "    \n",
    "    def conv_spktrain(self, Xx=np.linspace(-4,4,9),\n",
    "                      sigma=2,c=0,defaultST=True,spikeIn=[1,2,3],dt=0.02):\n",
    "        '''get smoothed spiketrain from spiketimes\n",
    "            **kwargs:\n",
    "            spikeTrain- 'False' if user wants self.ST (spiketimes)\n",
    "                        'True' if user wants to use a pre-allocated spiketrain\n",
    "            spikeIn-    use this optional kwarg iff spikeTrain==True\n",
    "        '''\n",
    "        if defaultST==True:\n",
    "            t = self.P[:,0]; dt = self.P[1,0]-self.P[0,0]; # time per frame\n",
    "            boolean_spk = np.logical_and(t[0] <= self.ST, self.ST <= t[-1])\n",
    "            spikes = self.ST[boolean_spk == True]\n",
    "            edgesT = np.linspace(t[0], t[-1], len(t)+1)\n",
    "            binnedSpikes, timeEdges = np.histogram(spikes, edgesT)\n",
    "            \n",
    "        elif defaultST==False:\n",
    "            binnedSpikes = spikeIn\n",
    "            \n",
    "        # convolve w/ gaussian membership function\n",
    "        filt = np.exp((-(Xx-c)**2)/(2*(sigma**2)))\n",
    "        fr = binnedSpikes/dt # rate (hz)\n",
    "        smooth_fr = np.convolve(binnedSpikes, filt, mode='same')\n",
    "        \n",
    "        return smooth_fr, binnedSpikes, filt, dt\n",
    "\n",
    "\n",
    "    \n",
    "    def get_speed(self):\n",
    "        '''get speed of the animal (cm*s^-2)'''\n",
    "        t=self.P[:,0]; x=self.P[:,1]; y=self.P[:,2];\n",
    "        ntime = len(t); v = np.zeros((ntime,1));\n",
    "        \n",
    "        for idx in range(1,ntime-1):\n",
    "            v[idx,0] = np.sqrt((x[idx+1]-x[idx-1])**2 + (y[idx+1]-y[idx-1])**2)/(t[idx+1]-t[idx-1])    \n",
    "        v[0,0] = v[1,0]; v[-1,0] = v[-2,0] # pad the array\n",
    "        \n",
    "        return v\n",
    "    \n",
    "    \n",
    "    \n",
    "    def speed_threshold(self,posgrid,ebgrid,spiketrain):\n",
    "        \n",
    "        v = self.get_speed()\n",
    "        maxspeed=50; minspeed=4\n",
    "        inbounds = np.logical_and((v<=maxspeed), (v>=minspeed))\n",
    "        inbounds = np.where(inbounds==True); inbounds = inbounds[0]\n",
    "        posgrid = posgrid[inbounds,:]\n",
    "        ebgrid = ebgrid[inbounds,:]\n",
    "        spiketrain = spiketrain[inbounds]\n",
    "        \n",
    "        return posgrid, ebgrid, spiketrain\n",
    "    \n",
    "    \n",
    "    \n",
    "    def squish_statemat(self, spiketrain, stateIn, modelType='PE'):\n",
    "        '''squish state matrix for 2-variable model (P+EB)\n",
    "            inputs- spiketrain is the speed-thresholded spiketrain'''\n",
    "        \n",
    "        if modelType == 'PE':\n",
    "            posgrid = stateIn[0]; ebgrid = stateIn[1]\n",
    "            ntime,nbins_eb = np.shape(ebgrid)\n",
    "            _,nbins_p = np.shape(posgrid)\n",
    "            A = np.zeros((ntime, nbins_p+nbins_eb)) #P+EB\n",
    "            A[:,0:nbins_p] = posgrid; A[:,nbins_p:] = ebgrid\n",
    "            df=pd.DataFrame(A)\n",
    "            \n",
    "            # name columns & get expression\n",
    "            colnames = [];\n",
    "            expr = 'y ~ '\n",
    "            \n",
    "            for i in range(nbins_p):\n",
    "                val = str(i);\n",
    "                expr = expr + 'P' + val + ' + '\n",
    "                colnames.append('P' + val)\n",
    "                \n",
    "            for i in range(nbins_eb-1):\n",
    "                val = str(i);\n",
    "                expr = expr + 'E' + val + ' + '\n",
    "                colnames.append('E' + val)\n",
    "            expr = expr + 'E9'\n",
    "            colnames.append('E9')\n",
    "            df.columns = colnames\n",
    "            \n",
    "        elif modelType == 'P':\n",
    "            ntime,nbins = np.shape(stateIn)\n",
    "            df = pd.DataFrame(stateIn)\n",
    "            colnames = [];\n",
    "            expr = 'y ~ '\n",
    "            \n",
    "            for i in range(nbins-1):\n",
    "                val = str(i);\n",
    "                expr = expr + 'P' + val + ' + '\n",
    "                colnames.append('P' + val)\n",
    "            expr = expr + 'P99'\n",
    "            colnames.append('P99')\n",
    "            df.columns = colnames\n",
    "            \n",
    "        elif modelType == 'E':\n",
    "            ntime,nbins = np.shape(stateIn)\n",
    "            df = pd.DataFrame(stateIn)\n",
    "            colnames = [];\n",
    "            expr = 'y ~ '\n",
    "            \n",
    "            for i in range(nbins-1):\n",
    "                val = str(i);\n",
    "                expr = expr + 'E' + val + ' + '\n",
    "                colnames.append('E' + val)\n",
    "            expr = expr + 'E9'\n",
    "            colnames.append('E9')\n",
    "            df.columns = colnames\n",
    "            \n",
    "        else:\n",
    "            print('Error: model type must be \"P\", \"E\", or \"PE\"')\n",
    "            \n",
    "        # if you want to do a 20-80 test-train split\n",
    "        # note: make this an option\n",
    "        mask = np.random.rand(len(df)) < 0.8\n",
    "        df_train = df[mask]; df_test = df[~mask]\n",
    "        \n",
    "        # insert [raw] spiketrain into dataframe\n",
    "        df.insert(0, 'y', spiketrain)\n",
    "        \n",
    "        return df,expr\n",
    "    \n",
    "    \n",
    "    \n",
    "    def kfoldSplit(self,nfolds=10):\n",
    "        '''train-test split for k-fold xval\n",
    "            each section is ~1 min'''\n",
    "        \n",
    "        _, spiketrain, _, dt = self.conv_spktrain()\n",
    "        \n",
    "        # calculate number of chunks given session length\n",
    "        nmins = (len(spiketrain)*dt)/60\n",
    "        nchunks = int(round(nmins/nfolds))\n",
    "        nsections = int(nchunks*nfolds)\n",
    "        \n",
    "        # grab indices for k-fold splitting\n",
    "        kfoldIdx = {}\n",
    "        howLong = np.zeros(nfolds)\n",
    "        edges = np.round(np.linspace(1,len(spiketrain)+1,nsections+1))\n",
    "        \n",
    "        for k in range(nfolds):\n",
    "            test_ind = np.floor(np.linspace(int(edges[k]),\n",
    "                    (int(edges[k+1])-1),\n",
    "                    (int(edges[k+1])-1)-int(edges[k])))\n",
    "            \n",
    "            for s in range(1,nchunks):\n",
    "                ind = np.floor(np.linspace(int(edges[k+s*nfolds]),\n",
    "                                  (int(edges[k+s*nfolds+1])-1),\n",
    "                                  (int(edges[k+s*nfolds+1]))-int(edges[k+s*nfolds])))\n",
    "                \n",
    "                test_ind = np.append(test_ind,ind)\n",
    "                \n",
    "            kfoldIdx[k] = test_ind\n",
    "            howLong[k] = len(test_ind)\n",
    "\n",
    "        minArrLen = int(np.min(howLong))\n",
    "        \n",
    "        for k in range(nfolds):\n",
    "            kfoldIdx[k] = kfoldIdx[k][0:minArrLen] # adjust arr. len (w/in .02 s)\n",
    "        \n",
    "        kfoldIdx_df = pd.DataFrame.from_dict(kfoldIdx)\n",
    "        kfoldIdx_df = kfoldIdx_df.astype(int) # for idxing purposes\n",
    "        \n",
    "        return kfoldIdx, kfoldIdx_df\n",
    "    \n",
    "    \n",
    "    \n",
    "    def kfoldOptim(self,kfoldIdx_df,statemat,modelType='PE'):\n",
    "        '''kfoldIdx_df can be retrieved from self.kfoldSplit()'''\n",
    "        \n",
    "        # intialize output structures\n",
    "        _,nfolds=np.shape(kfoldIdx_df)\n",
    "        k_vec = np.arange(nfolds)\n",
    "        kres = {}\n",
    "        train_y = {}\n",
    "        train_x = {}\n",
    "        test_y = {}\n",
    "        test_x = {}\n",
    "        train_y_raw = {}\n",
    "        test_y_raw = {}\n",
    "        \n",
    "        for foldnum in range(nfolds):\n",
    "            k_vec_train = np.delete(k_vec, np.where(k_vec == foldnum))\n",
    "            idx_test = kfoldIdx_df.loc[:,foldnum].to_numpy()\n",
    "            idx_train = []\n",
    "            \n",
    "            # squeeze other nfolds-1 folds into one vector\n",
    "            for i,v in enumerate(k_vec_train):\n",
    "                nextRow = kfoldIdx_df.loc[:,v].to_numpy()\n",
    "                idx_train = np.append(idx_train,nextRow) \n",
    "            idx_train = idx_train.astype(int)\n",
    "            \n",
    "            # train-test statemats\n",
    "            df_test = statemat.loc[idx_test,:].dropna()\n",
    "            df_train = statemat.loc[idx_train,:].dropna()\n",
    "            y_test_raw = df_test['y'].to_numpy(dtype='int64')\n",
    "            y_train_raw = df_train['y'].to_numpy(dtype='int64')\n",
    "            \n",
    "            # smooth firing rates\n",
    "            y_test, _, _, _ = self.conv_spktrain(defaultST=False,spikeIn=y_test_raw)\n",
    "            y_train, _, _, _ = self.conv_spktrain(defaultST=False,spikeIn=y_train_raw) \n",
    "            \n",
    "            # put smoothed firing rates back into dataframe\n",
    "            df_test[df_test.columns[0]] = y_test\n",
    "            df_train[df_train.columns[0]] = y_train\n",
    "            \n",
    "            # test/train arrays\n",
    "            X_test = df_test[df_test.columns[1:]].to_numpy(); \n",
    "            y_test = df_test[df_test.columns[0]].to_numpy()\n",
    "            X_train = df_train[df_train.columns[1:]].to_numpy()\n",
    "            y_train = df_train[df_train.columns[0]].to_numpy()\n",
    "            \n",
    "            # set some initial parameters\n",
    "            M,n = np.shape(X_train)\n",
    "            w_0 = 1e-3*np.ones((n, ))\n",
    "            b_0 = 1\n",
    "            # alpha = 0.001 (can't remember when we use this)\n",
    "            \n",
    "            # get parameters & jacobian (1st order derivatives of loss fn)\n",
    "            data,param = self.getDataParam(X_train,y_train,w_0,b_0,modelType)\n",
    "            \n",
    "            # not being used ** (depreciated right now)\n",
    "            # jac = self.grad(param,X_train,y_train)\n",
    "            \n",
    "            # optimize loss function\n",
    "            res = self.bfgs(data,param)\n",
    "            \n",
    "            # package outputs for each fold\n",
    "            kres[foldnum] = res\n",
    "            train_y[foldnum] = y_train; test_y[foldnum] = y_test\n",
    "            train_x[foldnum] = X_train; test_x[foldnum] = X_test\n",
    "            train_y_raw[foldnum] = y_train_raw; \n",
    "            test_y_raw[foldnum] = y_test_raw; \n",
    "            \n",
    "        return kres,train_y, test_y, train_x, test_x, train_y_raw, test_y_raw\n",
    "    \n",
    "    \n",
    "    \n",
    "    def init_params(self,whichVars={'P', 'E'}):\n",
    "        \n",
    "        if whichVars == {'P', 'E'}: init_param = 1e-3*np.random.randn(110, 1);\n",
    "        if whichVars == {'P'}: init_param = 1e-3*np.random.randn(100, 1);\n",
    "        if whichVars == {'E'}: init_param = 1e-3*np.random.randn(10, 1);\n",
    "        \n",
    "        return init_param\n",
    "    \n",
    "    \n",
    "    def getDataParam(self,x,y,w,b,modelType='PE'):\n",
    "        '''put param & data in a dictionary'''\n",
    "        \n",
    "        param = np.append(b,w)\n",
    "        data =  ((x, y, modelType))\n",
    "        \n",
    "        return data,param\n",
    "    \n",
    "    \n",
    "    def get_rate(self,x,w,b):\n",
    "        '''conditional intensity function'''\n",
    "        \n",
    "        # note: not normalized by dt (not in Hz)\n",
    "        y_hat = np.exp(x @ w + b)\n",
    "        \n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "    def loss(self,param,x,y,modelType):\n",
    "        '''objective function'''\n",
    "        # roughness regularizer weights\n",
    "        b_pos = 8e0; b_eb = 5e1;\n",
    "        \n",
    "        M, n = x.shape\n",
    "        \n",
    "        # predicted firing rate\n",
    "        y_hat = np.exp(x @ param[1:] + param[0])\n",
    "        \n",
    "        # compute jacobian (gradient)\n",
    "        dw = (x.T @ (y_hat - y)) / M\n",
    "        db = (y_hat - y).mean()\n",
    "        jac = dw; jac=np.append(jac,db);\n",
    "        \n",
    "        \n",
    "        ## penalize objective fn & gradient\n",
    "        if modelType == 'P':\n",
    "            J_pos, J_pos_g, J_pos_h = self.rough_penalty(param,b_pos,vartype='2D')\n",
    "            \n",
    "            y_hat += J_pos\n",
    "            jac += J_pos_g\n",
    "            \n",
    "        elif modelType == 'E':\n",
    "            J_eb, J_eb_g, J_eb_h = self.rough_penalty(param,b_eb,vartype='1D-circ')\n",
    "            \n",
    "            y_hat += J_eb\n",
    "            jac += J_eb_g\n",
    "            \n",
    "        elif modelType == 'PE':\n",
    "            J_pos, J_pos_g, J_pos_h = self.rough_penalty(param[:100],b_pos,vartype='2D')\n",
    "            J_eb, J_eb_g, J_eb_h = self.rough_penalty(param[100:],b_eb,vartype='1D-circ')\n",
    "\n",
    "            y_hat += J_pos\n",
    "            y_hat += J_eb\n",
    "            jac += J_pos_g\n",
    "            jac += J_eb_g\n",
    "                    \n",
    "        else:\n",
    "            print('error: enter valid model type (\"E\", \"P\", \"PE\")')\n",
    "        \n",
    "        #negative log likelihood for possion where yhat is lambda\n",
    "        error = (y_hat - np.log(y_hat) * y).mean()\n",
    "        \n",
    "        return [error,jac]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def bfgs(self,data,param):\n",
    "        'minimize loss function w/ L-BFGS-B'\n",
    "        \n",
    "        res = sp.optimize.minimize(self.loss, x0=param, args=data, method='L-BFGS-B', jac=True)\n",
    "        # options={'gtol': 1e-6, 'disp': True}) # add options \n",
    "        \n",
    "        return res\n",
    "    \n",
    "    \n",
    "    \n",
    "    def rough_penalty(self,param,beta,vartype='1D-circ'):\n",
    "        '''computes roughness penalty\n",
    "\n",
    "        inputs:\n",
    "        >> vartype:\n",
    "            >> '2D', '1D', or '1D-circ'\n",
    "\n",
    "        returns:\n",
    "        >> J: penalty term for objective function\n",
    "        >> J_g: penalty term for gradient (1st order derivatives)\n",
    "        >> J_h: penalty term for Hessian (2nd order derivatives)\n",
    "\n",
    "        '''\n",
    "        param = param[1:] # remove the bias term\n",
    "        numParam = len(param)\n",
    "\n",
    "        if vartype.__contains__('1D'):\n",
    "            data_diag = np.ones(int(numParam))\n",
    "            data_diag = [-data_diag,data_diag]\n",
    "            diags_diag = np.array([0,1]) # diagonals to set\n",
    "            m,n = int(numParam)-1, int(numParam) # shape of resulting matrix\n",
    "            D1 = spdiags(data_diag, diags_diag, m, n).toarray()\n",
    "            DD1 = D1.T @ D1\n",
    "\n",
    "            if vartype.__contains__('circ'):\n",
    "                # to correct the smoothing across first/last bin\n",
    "                DD1[0,:] = np.roll(DD1[1,:],((0, -1)))\n",
    "                DD1[-1,:] = np.roll(DD1[-1,:],((0, 1)))\n",
    "\n",
    "            # penalty terms\n",
    "            J = beta * 0.5 * param.T @ DD1 @ param\n",
    "            J_g = beta * DD1 @ param\n",
    "            J_h = beta * DD1\n",
    "\n",
    "        elif vartype.__contains__('2D'):\n",
    "\n",
    "            data_diag = np.ones(int(np.sqrt(numParam)))\n",
    "            data_diag = [-data_diag,data_diag]\n",
    "            diags_diag = np.array([0,1]) # diagonals to set\n",
    "            m,n = int(np.sqrt(numParam))-1, int(np.sqrt(numParam)) # shape of resulting matrix\n",
    "            D1 = spdiags(data_diag, diags_diag, m, n).toarray()\n",
    "            DD1 = D1.T @ D1\n",
    "\n",
    "            M1 = np.kron(np.eye(int(np.sqrt(numParam))),DD1)\n",
    "            M2 = np.kron(DD1,np.eye(int(np.sqrt(numParam))))\n",
    "            M = (M1 + M2)\n",
    "\n",
    "            J = beta * 0.5 * param.T @ M @ param\n",
    "            J_g = beta * M @ param\n",
    "            J_h = beta * M \n",
    "        \n",
    "        # add the bias term for the gradient *\n",
    "        # @note: this has not been done for the hessian (but we will need\n",
    "        # to if we want to use it)\n",
    "        J_g = np.concatenate((np.zeros(1), J_g))\n",
    "        \n",
    "        return J, J_g, J_h\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_stats(self, y, y_hat):\n",
    "        # compare between test fr and model fr\n",
    "        sse = np.sum((y_hat-y)**2);\n",
    "        sst = sum((y-np.mean(y))**2);\n",
    "        varExplain_test = 1-(sse/sst)\n",
    "        r, pval_r = stats.pearsonr(y,y_hat)\n",
    "        return sse, sst, varExplain_test, r, pval_r\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_testFit(self,kres,train_y,test_y,train_x,test_x,train_y_raw,test_y_raw):\n",
    "        '''get statistics for model fit'''\n",
    "        \n",
    "        # initialize output structures\n",
    "        yhatDict={}\n",
    "        nfolds=len(kres);\n",
    "        sse=np.zeros(nfolds); sst=np.zeros(nfolds)\n",
    "        varExplain_test=np.zeros(nfolds); pearson_r=np.zeros(nfolds)\n",
    "        pearson_pval=np.zeros(nfolds); funval=np.zeros(nfolds)\n",
    "        llh=np.zeros(nfolds)\n",
    "        \n",
    "        for fold in range(nfolds):\n",
    "            # predict model output on *test* data\n",
    "            yhat_raw = self.get_rate(test_x[fold],kres[fold].x[1:],kres[fold].x[0]) # not normalized\n",
    "            yhat, _, _, _ = self.conv_spktrain(defaultST=False,spikeIn=yhat_raw) # normalized by dt (hz)\n",
    "            \n",
    "            # compare between test and model rates\n",
    "            sse[fold] = np.sum((yhat-test_y[fold])**2); #sse\n",
    "            sst[fold]= sum((test_y[fold]-np.mean(test_y[fold]))**2); #sst\n",
    "            varExplain_test[fold] = 1-(sse[fold]/sst[fold]) #varExplained_test\n",
    "            pearson_r[fold],pearson_pval[fold] = stats.pearsonr(test_y[fold],yhat) #pearsonsR,p-val\n",
    "            funval[fold] = kres[fold].fun\n",
    "            yhatDict[fold] = yhat\n",
    "            \n",
    "            #compute llh increase from \"mean firing rate model\"\n",
    "            # NO SMOOTHING is used here\n",
    "            bestp = kres[fold]['x'] # best parameters\n",
    "            n = test_y_raw[fold]\n",
    "            arrFactorial = np.vectorize(math.factorial) # array-wise factorial fn\n",
    "            meanFR_test = np.nanmean(n)\n",
    "\n",
    "            # format the state matrix\n",
    "            b = np.ones((len(test_x[fold]),len(bestp)))\n",
    "            b[:,1:] = test_x[fold]\n",
    "            # get rate of 'mean fr model'\n",
    "            r = np.exp(b @ bestp) # predicted rate (not normalized by dt)\n",
    "            \n",
    "            # compute log-likelihood value for the test data\n",
    "            log_llh_test_model = np.nansum(r-n*np.log(r)+np.log(arrFactorial(n)))/np.sum(n);\n",
    "            log_llh_test_mean = np.nansum(meanFR_test-n*np.log(meanFR_test)+np.log(arrFactorial(n)))/np.sum(n);\n",
    "            log_llh_test = (-log_llh_test_model + log_llh_test_mean);\n",
    "            log_llh_test = np.log(2)*log_llh_test;\n",
    "            llh[fold] = log_llh_test\n",
    "            \n",
    "            # akaike info criterion\n",
    "            # log is undefined b/c llh is negative\n",
    "            # AIC = 2*(len(bestp))-2*np.log(log_llh_test);\n",
    "            \n",
    "        # dictionary of statistics describing fit of test data to model\n",
    "        testfit = {\n",
    "            'llh_test': llh,\n",
    "            'sse': sse,\n",
    "            'sst': sst,\n",
    "            'varEx': varExplain_test,\n",
    "            'pearson_r': pearson_r,\n",
    "            'pearson_pval': pearson_pval,\n",
    "            'funval': funval,\n",
    "            'yhat': yhatDict\n",
    "        }\n",
    "        return testfit\n",
    "    \n",
    "    \n",
    "    \n",
    "    def findBestModel(self,allModels,labelDict):\n",
    "        '''modelDict is defined in the script below (incorporate)'''\n",
    "        \n",
    "        numModels = len(allModels)\n",
    "        llh = np.zeros(numModels)\n",
    "        \n",
    "        for model in range(numModels):\n",
    "            T = allModels[model]['testfit']\n",
    "            llh[model] = np.nanmean(T['llh_test'])\n",
    "        bestModel=np.where(llh==np.max(llh))[0][0]\n",
    "        \n",
    "        print('best model: ' + labelDict[bestModel])\n",
    "        \n",
    "        return llh, bestModel\n",
    "    \n",
    "    \n",
    "    \n",
    "    def plot_llh(self,allModels,labelDict):\n",
    "        '''plot cross-validated\n",
    "        log-likelihood values for all models'''\n",
    "        \n",
    "        # plot log-likelihood values for each model\n",
    "        numModels = len(allModels)\n",
    "        nfolds = len(allModels[0]['kres'].keys())\n",
    "        \n",
    "        llh = {}; whichModel = {}\n",
    "        \n",
    "        for model in range(numModels):\n",
    "            a = allModels[model]['testfit']['llh_test']\n",
    "            a = a[None].T\n",
    "            llh[model] = a\n",
    "            whichModel[model] = np.ones((nfolds,1))*model\n",
    "\n",
    "        llh = np.vstack((llh[0], llh[1], llh[2]))\n",
    "        whichModel = np.vstack((whichModel[0], whichModel[1], whichModel[2]))\n",
    "        df = pd.DataFrame(np.hstack((llh,whichModel)), columns = ['llh','whichModel'])\n",
    "        ax = sns.swarmplot(x=\"whichModel\", y=\"llh\", data=df)\n",
    "        \n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization parameters\n",
    "plt.rcParams['figure.figsize'] = (4,2)\n",
    "plt.rc('axes', labelsize=10); plt.rc('axes', titlesize=10)\n",
    "plt.style.use('ggplot'); plt.rc('font', size=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load & format sample data\n",
    "filepath = 'sampleData.mat'\n",
    "mat = scipy.io.loadmat(filepath)\n",
    "ST = mat['ST']; P = mat['P']; hd = mat['hd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run glm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing model # 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17145\\.conda\\envs\\tdt4195\\lib\\site-packages\\pandas\\core\\indexing.py:1418: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 81 is different from 99)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-121-ee8bc9fac0f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m# optimize model parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mkres\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_raw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y_raw\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkfoldOptim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkfoldIdx_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstatemat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodelType\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabelDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# check the model fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-118-01eabd56fd10>\u001b[0m in \u001b[0;36mkfoldOptim\u001b[1;34m(self, kfoldIdx_df, statemat, modelType)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;31m# optimize loss function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 305\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbfgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[1;31m# package outputs for each fold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-118-01eabd56fd10>\u001b[0m in \u001b[0;36mbfgs\u001b[1;34m(self, data, param)\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[1;34m'minimize loss function w/ L-BFGS-B'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'L-BFGS-B'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m         \u001b[1;31m# options={'gtol': 1e-6, 'disp': True}) # add options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tdt4195\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'l-bfgs-b'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m--> 600\u001b[1;33m                                 callback=callback, **options)\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[1;32m~\\.conda\\envs\\tdt4195\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m             \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tdt4195\\lib\\site-packages\\scipy\\optimize\\lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tdt4195\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tdt4195\\lib\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-118-01eabd56fd10>\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, param, x, y, modelType)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmodelType\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'PE'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m             \u001b[0mJ_pos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mJ_pos_g\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mJ_pos_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrough_penalty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb_pos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvartype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'2D'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m             \u001b[0mJ_eb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mJ_eb_g\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mJ_eb_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrough_penalty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb_eb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvartype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'1D-circ'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-118-01eabd56fd10>\u001b[0m in \u001b[0;36mrough_penalty\u001b[1;34m(self, param, beta, vartype)\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mM1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mM2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m             \u001b[0mJ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mM\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    451\u001b[0m             \u001b[0mJ_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mM\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m             \u001b[0mJ_h\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 81 is different from 99)"
     ]
    }
   ],
   "source": [
    "# initialize class instance\n",
    "# g = glmmod.glm(ST,P,hd)\n",
    "g = glm(ST,P,hd)\n",
    "\n",
    "# prepare the data\n",
    "posgrid_raw,bins = g.pos_map(nbins=10)\n",
    "ebgrid_raw,bins = g.eb_map(nbins=10, rp=[75,75])\n",
    "smooth_fr, raw_spktrn, filt, dt = g.conv_spktrain() # get spiketrain\n",
    "posgrid,ebgrid,spiketrain = g.speed_threshold(posgrid_raw,ebgrid_raw,raw_spktrn)\n",
    "\n",
    "# dictionaries with info about each model\n",
    "stateDict = {\n",
    "    0: [posgrid,ebgrid],\n",
    "    1: posgrid,\n",
    "    2: ebgrid\n",
    "}\n",
    "\n",
    "labelDict = {\n",
    "    0: 'PE',\n",
    "    1: 'P',\n",
    "    2: 'E'\n",
    "}\n",
    "\n",
    "allModels = {}\n",
    "numModels = 2\n",
    "\n",
    "# get test/train indices (same for each model)\n",
    "kfoldIdx, kfoldIdx_df = g.kfoldSplit(nfolds=10)\n",
    "\n",
    "# iterate through all models of interest\n",
    "for model in range(numModels):\n",
    "    mt = labelDict[model]\n",
    "    \n",
    "    print('processing model # ' + str(model) + '(' + mt + ') ...')\n",
    "    \n",
    "    modelDict = {}\n",
    "\n",
    "    # get state matrix\n",
    "    stateIn = stateDict[model]\n",
    "    statemat, expr = g.squish_statemat(spiketrain, stateIn, modelType=mt)\n",
    "\n",
    "    # optimize model parameters\n",
    "    kres,train_y, test_y, train_x, test_x, train_y_raw, test_y_raw= g.kfoldOptim(kfoldIdx_df,statemat,modelType=labelDict[model])\n",
    "\n",
    "    # check the model fit\n",
    "    testfit = g.get_testFit(kres,train_y,test_y,train_x,test_x,train_y_raw,test_y_raw)\n",
    "\n",
    "    modelDict['kfoldIdx'] = kfoldIdx_df\n",
    "    modelDict['kres'] = kres\n",
    "    modelDict['train_y'] = train_y\n",
    "    modelDict['train_x'] = train_x\n",
    "    modelDict['test_y'] = test_y\n",
    "    modelDict['test_x'] = test_x\n",
    "    modelDict['train_y_raw'] = train_y_raw\n",
    "    modelDict['test_y_raw'] = test_y_raw\n",
    "    modelDict['testfit'] = testfit\n",
    "\n",
    "    # save in allModels dictionary\n",
    "    allModels[model] = modelDict\n",
    "\n",
    "    llh, bestModel = g.findBestModel(allModels,labelDict)\n",
    "\n",
    "    g.plot_llh(allModels,labelDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute penalization terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 5e1 # for example\n",
    "XX = allModels[1]['test_x'][0] # state matrix\n",
    "_,numvar = np.shape(XX)\n",
    "init_param = 1e-3*np.random.randn(numvar) # from random normal distrib\n",
    "param = init_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "J, J_g, J_h = rough_penalty(param,beta,vartype='1D-circ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for penalization stuff\n",
    "if smooth:\n",
    "    fpen,fgrad = penalize(params,X,spike_train,smoothers)\n",
    "    f += fpen\n",
    "    grad += fgrad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
